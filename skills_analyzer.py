# skills_analyzer.py â€” æŠ€æœ¯æŠ€èƒ½ä¸è·¨å­¦ç§‘çŸ¥è¯†åˆ†ææ¨¡å—

import json
import os
import re
import threading
from collections import Counter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

from config import GEMINI_MODEL, DOMAIN, MIN_PAPERS_FOR_ROADMAP, SKILL_CACHE_PATH


# â”€â”€ Promptï¼šæŠ€èƒ½æå– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SKILL_EXTRACTION_SYSTEM_PROMPT = """You are a research skill analyst. Given a structured paper summary, extract ALL technical
and disciplinary knowledge required to understand, reproduce, or extend this work.

Be specific and granular. Do not generalize "machine learning" when you can say "sparse autoencoder".
For interdisciplinary requirements, identify the source discipline explicitly.

Output ONLY valid JSON:
{
  "ml_methods": ["string"],
  "math_foundations": ["string"],
  "programming_tools": ["string"],
  "domain_knowledge": ["string"],
  "interdisciplinary": [
    {
      "discipline": "string (e.g. Neuroscience, Psychology, Linguistics, Physics)",
      "specific_knowledge": "string (e.g. working memory models, Gestalt principles)",
      "why_needed": "string (1 sentence)"
    }
  ],
  "prerequisite_papers_or_concepts": ["string"],
  "difficulty_level": "undergraduate | graduate | postdoc"
}"""

SKILL_EXTRACTION_USER_TEMPLATE = """Paper Summary:
Title: {title}
Sub-domain: {sub_domain}
Problem: {problem_statement}
Method: {method_summary}
Keywords: {method_keywords}
Contributions: {contributions}

Extract all required skills now."""


# â”€â”€ Promptï¼šå­¦ä¹ è·¯çº¿å›¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ROADMAP_SYSTEM_PROMPT = """You are a research mentor. Given a frequency-ranked list of skills required across recent papers
in a research domain, design a practical learning roadmap for someone with a CS undergraduate background.

Output ONLY valid JSON â€” an array of stages:
[
  {
    "stage": 1,
    "title": "string (e.g. 'Mathematical Foundations')",
    "duration_weeks": int,
    "skills": ["string"],
    "resources_type": ["string (e.g. 'linear algebra textbook', 'PyTorch tutorial')"],
    "milestone": "string (what you can do after this stage)",
    "interdisciplinary_intro": "string or null (if this stage requires cross-discipline knowledge, name it)"
  }
]

Rules:
- Maximum 5 stages
- Be realistic about duration
- Stage 1 should always be prerequisites
- Last stage should be "reading seminal papers in the field"
- Only include skills that appeared in the actual paper data provided"""

ROADMAP_USER_TEMPLATE = """Domain: {domain}
Total papers analyzed: {count}

Skills ranked by frequency (must_have > important > good_to_have):

Must-have skills (appear in >60% of papers):
{must_have}

Important skills (appear in 30-60% of papers):
{important}

Good-to-have skills (appear in <30% of papers):
{good_to_have}

Interdisciplinary requirements:
{interdisciplinary}

Generate the learning roadmap now."""


# â”€â”€ æŠ€èƒ½è§„èŒƒåŒ–ä¸å»é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SKILL_SYNONYMS = {
    "pytorch": "deep learning frameworks",
    "tensorflow": "deep learning frameworks",
    "keras": "deep learning frameworks",
    "jax": "deep learning frameworks",
    "deep learning frameworks (e.g., pytorch, tensorflow)": "deep learning frameworks",
    "deep learning framework": "deep learning frameworks",
    "linear algebra (vectors, matrices, eigenvalues)": "linear algebra",
    "linear algebra (vectors, matrices)": "linear algebra",
    "calculus (multivariate)": "calculus",
    "multivariate calculus": "calculus",
    "numpy": "numpy/scipy",
    "scipy": "numpy/scipy",
    "python": "python programming",
    "python programming language": "python programming",
    "python 3": "python programming",
    "machine learning": "machine learning fundamentals",
    "ml": "machine learning fundamentals",
    "deep learning": "deep learning fundamentals",
    "dl": "deep learning fundamentals",
    "neural networks": "deep learning fundamentals",
    "neural network": "deep learning fundamentals",
    "convolutional neural networks": "convolutional neural networks (cnn)",
    "cnn": "convolutional neural networks (cnn)",
    "cnns": "convolutional neural networks (cnn)",
    "recurrent neural networks": "recurrent neural networks (rnn)",
    "rnn": "recurrent neural networks (rnn)",
    "transformers": "transformer architecture",
    "transformer": "transformer architecture",
    "attention mechanism": "transformer architecture",
    "self-attention": "transformer architecture",
    "gpu programming": "gpu/cuda programming",
    "cuda": "gpu/cuda programming",
    "git": "version control (git)",
    "github": "version control (git)",
}


def normalize_and_deduplicate_skills(skills_list: list[str]) -> list[str]:
    """
    è§„èŒƒåŒ–å¹¶å»é‡æŠ€èƒ½åˆ—è¡¨ï¼š
    1. å°å†™åŒ–
    2. å»æ‰æ‹¬å·å†…çš„è¯´æ˜æ–‡å­—ï¼ˆå¦‚ "linear algebra (vectors, matrices)" â†’ "linear algebra"ï¼‰
    3. åº”ç”¨åŒä¹‰è¯æ˜ å°„
    4. é›†åˆå»é‡
    """
    seen = set()
    result = []
    for skill in skills_list:
        s = skill.lower().strip()
        # å»æ‰æ‹¬å·å†…çš„è¡¥å……è¯´æ˜ï¼ˆä½†å…ˆæŸ¥ä¸€æ¬¡å®Œæ•´ç‰ˆåŒä¹‰è¯ï¼‰
        if s in SKILL_SYNONYMS:
            s = SKILL_SYNONYMS[s]
        else:
            s = re.sub(r'\s*\(.*?\)', '', s).strip()
            if s in SKILL_SYNONYMS:
                s = SKILL_SYNONYMS[s]
        if s and s not in seen:
            seen.add(s)
            result.append(s)
    return result


# â”€â”€ 1. æŠ€èƒ½æå–ï¼ˆå«ç¼“å­˜ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_cache_lock = threading.Lock()


def _load_skill_cache() -> dict:
    """ä»ç£ç›˜åŠ è½½æŠ€èƒ½ç¼“å­˜"""
    if os.path.exists(SKILL_CACHE_PATH):
        try:
            with open(SKILL_CACHE_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, OSError):
            pass
    return {}


def _save_skill_cache(cache: dict):
    """å°†æŠ€èƒ½ç¼“å­˜å†™å…¥ç£ç›˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    with _cache_lock:
        try:
            with open(SKILL_CACHE_PATH, "w", encoding="utf-8") as f:
                json.dump(cache, f, ensure_ascii=False, indent=2)
        except OSError as e:
            print(f"[Skills] ç¼“å­˜å†™å…¥å¤±è´¥: {e}")


def _clean_json(raw: str) -> str:
    """é˜²å¾¡æ€§æ¸…æ´—ï¼šå»æ‰ markdown ä»£ç å›´æ """
    raw = raw.strip()
    if raw.startswith("```"):
        raw = raw.split("```")[1]
        if raw.startswith("json"):
            raw = raw[4:]
    return raw.strip()


def extract_skills_from_paper(paper: dict, llm: ChatGoogleGenerativeAI) -> dict | None:
    """
    ç”¨ Gemini æå–å•ç¯‡è®ºæ–‡æ‰€éœ€çš„æŠ€æœ¯ä¸è·¨å­¦ç§‘æŠ€èƒ½ã€‚
    ä¼˜å…ˆä»æœ¬åœ°ç¼“å­˜è¯»å–ï¼Œç¼“å­˜æœªå‘½ä¸­æ—¶è°ƒç”¨ LLM å¹¶å†™å…¥ç¼“å­˜ã€‚
    è¿”å›è§£æå¥½çš„ dictï¼Œå¤±è´¥æ—¶è¿”å› Noneã€‚
    """
    arxiv_id = paper.get("arxiv_id", "")

    # â”€â”€ ç¼“å­˜å‘½ä¸­æ£€æŸ¥ â”€â”€
    if arxiv_id:
        cache = _load_skill_cache()
        if arxiv_id in cache:
            print(f"[Skills] ç¼“å­˜å‘½ä¸­: {paper.get('title', '')[:50]}")
            return cache[arxiv_id]

    user_msg = SKILL_EXTRACTION_USER_TEMPLATE.format(
        title=paper.get("title", ""),
        sub_domain=paper.get("sub_domain", ""),
        problem_statement=paper.get("problem_statement", ""),
        method_summary=paper.get("method_summary", ""),
        method_keywords=", ".join(paper.get("method_keywords", [])),
        contributions="; ".join(paper.get("contributions", []) or []),
    )

    try:
        response = llm.invoke([
            SystemMessage(content=SKILL_EXTRACTION_SYSTEM_PROMPT),
            HumanMessage(content=user_msg),
        ])
        raw = _clean_json(response.content)
        extracted = json.loads(raw)
        extracted["_paper_title"] = paper.get("title", "")

        # â”€â”€ å†™å…¥ç¼“å­˜ â”€â”€
        if arxiv_id:
            cache = _load_skill_cache()
            cache[arxiv_id] = extracted
            _save_skill_cache(cache)
            print(f"[Skills] å·²ç¼“å­˜: {paper.get('title', '')[:50]}")

        return extracted
    except json.JSONDecodeError as e:
        print(f"[Skills] JSON è§£æå¤±è´¥: {paper.get('title', '')[:50]}... é”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"[Skills] æŠ€èƒ½æå–å¤±è´¥: {e}")
        return None


# â”€â”€ 2. æŠ€èƒ½èšåˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# 3 cleaner categories (replaces ml_methods/math_foundations/programming_tools/domain_knowledge)
_SKILL_CATEGORIES = ["core_theory", "applied_methods", "engineering_tools"]

# Mapping from LLM output keys â†’ new category
_LLM_KEY_TO_CATEGORY = {
    "math_foundations": "core_theory",
    "ml_methods": "core_theory",
    "domain_knowledge": "applied_methods",
    "programming_tools": "engineering_tools",
}

_MAX_PER_BUCKET = 8   # cap per tier per category
_MAX_TABLE_ROWS = 20  # max total rows rendered in markdown


def aggregate_skills(papers_skills: list[dict]) -> dict:
    """
    åˆå¹¶æ‰€æœ‰è®ºæ–‡çš„æŠ€èƒ½æå–ç»“æœï¼Œè§„èŒƒåŒ–å»é‡åæŒ‰å‡ºç°é¢‘ç‡åˆ†ç±»ã€‚

    è¿”å›:
        èšåˆåçš„æŠ€èƒ½å­—å…¸ï¼Œå« must_have / important / good_to_have /
        interdisciplinary_summary / dedup_stats
    """
    total = len(papers_skills)
    if total == 0:
        return {
            "must_have": {cat: [] for cat in _SKILL_CATEGORIES},
            "important": {cat: [] for cat in _SKILL_CATEGORIES},
            "good_to_have": {cat: [] for cat in _SKILL_CATEGORIES},
            "interdisciplinary_summary": [],
            "learning_roadmap": [],
            "dedup_stats": {"before": 0, "after": 0},
        }

    # â”€â”€ æŒ‰æ–°ç±»åˆ«ç»Ÿè®¡ï¼ˆè§„èŒƒåŒ–åï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    category_counters: dict[str, Counter] = {cat: Counter() for cat in _SKILL_CATEGORIES}
    total_raw = 0
    total_normalized_set: set[str] = set()

    for ps in papers_skills:
        for llm_key, target_cat in _LLM_KEY_TO_CATEGORY.items():
            raw_skills = ps.get(llm_key, [])
            total_raw += len(raw_skills)
            normed = normalize_and_deduplicate_skills(raw_skills)
            total_normalized_set.update(normed)
            for skill in normed:
                category_counters[target_cat][skill] += 1

    # â”€â”€ æŒ‰é¢‘ç‡åˆ†æ¡¶ï¼ˆ60% / 30% / <30%ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    must_have: dict[str, list] = {cat: [] for cat in _SKILL_CATEGORIES}
    important: dict[str, list] = {cat: [] for cat in _SKILL_CATEGORIES}
    good_to_have: dict[str, list] = {cat: [] for cat in _SKILL_CATEGORIES}

    for cat in _SKILL_CATEGORIES:
        for skill, count in category_counters[cat].most_common():
            pct = count / total
            entry = {"skill": skill, "frequency": count, "percentage": round(pct * 100)}
            if pct >= 0.6:
                if len(must_have[cat]) < _MAX_PER_BUCKET:
                    must_have[cat].append(entry)
            elif pct >= 0.3:
                if len(important[cat]) < _MAX_PER_BUCKET:
                    important[cat].append(entry)
            else:
                if len(good_to_have[cat]) < _MAX_PER_BUCKET:
                    good_to_have[cat].append(entry)

    # â”€â”€ è·¨å­¦ç§‘èšåˆï¼šåˆå¹¶åŒå­¦ç§‘æ¡ç›® + é¢‘ç‡ >= 2 è¿‡æ»¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    discipline_data: dict[str, dict] = {}
    for ps in papers_skills:
        for inter in ps.get("interdisciplinary", []):
            disc = inter.get("discipline", "Unknown").strip().title()
            if disc not in discipline_data:
                discipline_data[disc] = {"concepts": set(), "whys": [], "count": 0}
            discipline_data[disc]["count"] += 1
            concept = inter.get("specific_knowledge", "").strip()
            if concept:
                discipline_data[disc]["concepts"].add(concept)
            why = inter.get("why_needed", "").strip()
            if why and why not in discipline_data[disc]["whys"]:
                discipline_data[disc]["whys"].append(why)

    interdisciplinary_summary = []
    for disc, data in sorted(discipline_data.items(), key=lambda x: x[1]["count"], reverse=True):
        if data["count"] < 2:
            continue  # è¿‡æ»¤å•æ¬¡å‡ºç°çš„å­¦ç§‘
        interdisciplinary_summary.append({
            "discipline": disc,
            "frequency": data["count"],
            "percentage_of_papers": round(data["count"] / total * 100, 1),
            "key_concepts": sorted(data["concepts"]),
            "example_why_needed": data["whys"][0] if data["whys"] else "",
        })

    return {
        "must_have": must_have,
        "important": important,
        "good_to_have": good_to_have,
        "interdisciplinary_summary": interdisciplinary_summary,
        "learning_roadmap": [],
        "dedup_stats": {
            "before": total_raw,
            "after": len(total_normalized_set),
        },
    }


# â”€â”€ 3. å­¦ä¹ è·¯çº¿å›¾ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _format_skill_bucket(bucket: dict) -> str:
    """å°†ä¸€ä¸ªæŠ€èƒ½åˆ†æ¡¶æ ¼å¼åŒ–ä¸ºæ–‡æœ¬"""
    lines = []
    for cat in _SKILL_CATEGORIES:
        items = bucket.get(cat, [])
        if items:
            label = cat.replace("_", " ").title()
            skills_str = ", ".join(
                f"{s['skill']} ({s['frequency']}x)" if isinstance(s, dict) else str(s)
                for s in items
            )
            lines.append(f"  {label}: {skills_str}")
    return "\n".join(lines) if lines else "  (none)"


def generate_learning_roadmap(
    aggregated: dict,
    llm: ChatGoogleGenerativeAI = None,
    paper_count: int = 0,
) -> list[dict] | None:
    """
    åŸºäºèšåˆåçš„æŠ€èƒ½æ•°æ®ï¼Œè°ƒç”¨ Gemini ç”Ÿæˆå­¦ä¹ è·¯çº¿å›¾ã€‚

    å‚æ•°:
        aggregated: aggregate_skills çš„è¿”å›å€¼
        llm: LangChain LLM å®ä¾‹ï¼ˆä¸ºç©ºåˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
        paper_count: å®é™…è®ºæ–‡æ•°é‡ï¼Œä½äº MIN_PAPERS_FOR_ROADMAP æ—¶è·³è¿‡

    è¿”å›:
        è·¯çº¿å›¾ stage åˆ—è¡¨ï¼Œæˆ– Noneï¼ˆæ•°æ®ä¸è¶³ï¼‰
    """
    if paper_count < MIN_PAPERS_FOR_ROADMAP:
        print(
            f"[Skills] è®ºæ–‡æ•° ({paper_count}) < è·¯çº¿å›¾é˜ˆå€¼ ({MIN_PAPERS_FOR_ROADMAP})ï¼Œ"
            "è·³è¿‡è·¯çº¿å›¾ç”Ÿæˆã€‚"
        )
        return None

    if llm is None:
        llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0.3)

    inter_lines = []
    for item in aggregated.get("interdisciplinary_summary", []):
        inter_lines.append(
            f"  {item['discipline']} ({item['frequency']}x, "
            f"{item['percentage_of_papers']}%): {', '.join(item['key_concepts'][:3])}"
        )
    inter_str = "\n".join(inter_lines) if inter_lines else "  (none)"

    user_msg = ROADMAP_USER_TEMPLATE.format(
        domain=DOMAIN,
        count=sum(
            len(aggregated["must_have"].get(c, []))
            + len(aggregated["important"].get(c, []))
            + len(aggregated["good_to_have"].get(c, []))
            for c in _SKILL_CATEGORIES
        ),
        must_have=_format_skill_bucket(aggregated["must_have"]),
        important=_format_skill_bucket(aggregated["important"]),
        good_to_have=_format_skill_bucket(aggregated["good_to_have"]),
        interdisciplinary=inter_str,
    )

    print("[Skills] æ­£åœ¨ç”Ÿæˆå­¦ä¹ è·¯çº¿å›¾...")

    try:
        response = llm.invoke([
            SystemMessage(content=ROADMAP_SYSTEM_PROMPT),
            HumanMessage(content=user_msg),
        ])
        raw = _clean_json(response.content)
        roadmap = json.loads(raw)
        if isinstance(roadmap, list):
            return roadmap
        print("[Skills] è·¯çº¿å›¾æ ¼å¼ä¸æ­£ç¡®ï¼ŒæœŸæœ›æ•°ç»„")
        return []
    except json.JSONDecodeError as e:
        print(f"[Skills] è·¯çº¿å›¾ JSON è§£æå¤±è´¥: {e}")
        return []
    except Exception as e:
        print(f"[Skills] è·¯çº¿å›¾ç”Ÿæˆå¤±è´¥: {e}")
        return []


# â”€â”€ 4. Markdown æ¸²æŸ“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_CATEGORY_LABELS = {
    "core_theory": "Core Theory",
    "applied_methods": "Applied Methods",
    "engineering_tools": "Engineering Tools",
}


def render_skills_markdown(aggregated: dict) -> str:
    """
    å°†èšåˆåçš„æŠ€èƒ½æ•°æ®æ¸²æŸ“ä¸º Markdown ç« èŠ‚ã€‚
    æœ€å¤š 20 è¡ŒæŠ€èƒ½è¡¨ + åˆ†ç±»è§†è§‰åˆ†éš” + ç²¾ç®€è·¯çº¿å›¾ã€‚
    """
    lines = []
    lines.append("## 7. Skills & Learning Roadmap")
    lines.append("")

    # â”€â”€ å»é‡ç»Ÿè®¡ â”€â”€
    stats = aggregated.get("dedup_stats", {})
    if stats.get("before"):
        lines.append(
            f"*Skills normalized: {stats['before']} raw entries "
            f"â†’ {stats['after']} unique after deduplication*"
        )
        lines.append("")

    # â”€â”€ æŠ€èƒ½è¡¨æ ¼ï¼ˆæœ€å¤š _MAX_TABLE_ROWS è¡Œï¼‰ â”€â”€
    lines.append("### Required Technical Skills")
    lines.append("")
    lines.append("| Skill | Category | Priority | Frequency |")
    lines.append("| --- | --- | --- | --- |")

    # æ”¶é›†æ‰€æœ‰ must_have + important æ¡ç›®ï¼ŒæŒ‰é¢‘ç‡æ’åº
    all_rows = []
    good_to_have_count = 0
    for tier_name, tier_label in [
        ("must_have", "Must-have"),
        ("important", "Important"),
    ]:
        tier = aggregated.get(tier_name, {})
        for cat in _SKILL_CATEGORIES:
            cat_label = _CATEGORY_LABELS.get(cat, cat)
            for item in tier.get(cat, []):
                skill = item["skill"] if isinstance(item, dict) else str(item)
                freq = item.get("frequency", 0) if isinstance(item, dict) else 0
                pct = item.get("percentage", 0) if isinstance(item, dict) else 0
                all_rows.append((freq, skill, cat_label, tier_label, pct))

    # ç»Ÿè®¡ good_to_have æ•°é‡ï¼ˆä¸æ˜¾ç¤ºåœ¨è¡¨é‡Œï¼‰
    gth = aggregated.get("good_to_have", {})
    for cat in _SKILL_CATEGORIES:
        good_to_have_count += len(gth.get(cat, []))

    # æŒ‰é¢‘ç‡é™åºæ’åˆ—ï¼Œå–å‰ _MAX_TABLE_ROWS è¡Œ
    all_rows.sort(key=lambda r: r[0], reverse=True)
    shown = all_rows[:_MAX_TABLE_ROWS]
    overflow = len(all_rows) - len(shown) + good_to_have_count

    # æŒ‰ç±»åˆ«åˆ†ç»„è¾“å‡ºï¼ˆè§†è§‰åˆ†éš”ï¼‰
    current_cat = None
    for freq, skill, cat_label, tier_label, pct in shown:
        if cat_label != current_cat:
            current_cat = cat_label
            # ç±»åˆ«åˆ†éš”è¡Œï¼ˆç²—ä½“ç±»åˆ«åï¼Œåˆå¹¶åˆ—ï¼‰
            lines.append(f"| **{cat_label}** | | | |")
        freq_str = f"{freq}x ({pct}%)" if pct else f"{freq}x"
        lines.append(f"| {skill} | {cat_label} | {tier_label} | {freq_str} |")

    if overflow > 0:
        lines.append(f"| *... and {overflow} additional skills (good-to-have)* | | | |")

    lines.append("")

    # â”€â”€ è·¨å­¦ç§‘éœ€æ±‚ â”€â”€
    inter = aggregated.get("interdisciplinary_summary", [])
    if inter:
        lines.append("### Interdisciplinary Requirements")
        lines.append("")
        for item in inter:
            disc = item.get("discipline", "Unknown")
            freq = item.get("frequency", 0)
            total_pct = item.get("percentage_of_papers", 0)
            concepts = ", ".join(item.get("key_concepts", [])[:5])
            why = item.get("example_why_needed", "")
            lines.append(
                f"> ğŸ§  **{disc}** (required in {freq} papers, {total_pct}%): "
                f"{concepts}"
            )
            if why:
                lines.append(f"> _{why}_")
            lines.append(">")
        lines.append("")

    # â”€â”€ å­¦ä¹ è·¯çº¿å›¾ï¼ˆç²¾ç®€ç‰ˆï¼šæ¯é˜¶æ®µæœ€å¤š 5 æ¡ï¼‰ â”€â”€
    roadmap = aggregated.get("learning_roadmap", [])
    if roadmap:
        lines.append("### Learning Roadmap")
        lines.append("")
        for stage in roadmap:
            num = stage.get("stage", "?")
            title = stage.get("title", "Untitled")
            weeks = stage.get("duration_weeks", "?")
            milestone = stage.get("milestone", "")
            inter_intro = stage.get("interdisciplinary_intro")

            lines.append(f"**Stage {num}: {title}** (~{weeks} weeks)")

            # åˆå¹¶ skills + resources ä¸ºç²¾ç®€è¦ç‚¹ï¼ˆæœ€å¤š 5 æ¡ï¼‰
            bullets = []
            for s in stage.get("skills", [])[:3]:
                bullets.append(s)
            for r in stage.get("resources_type", [])[:2]:
                bullets.append(f"Resource: {r}")
            for b in bullets[:5]:
                lines.append(f"- {b}")

            if milestone:
                lines.append(f"  âœ… *{milestone}*")
            if inter_intro:
                lines.append(f"  ğŸ”— *{inter_intro}*")
            lines.append("")

    return "\n".join(lines)


if __name__ == "__main__":
    # å»é‡æµ‹è¯•
    test_input = [
        "PyTorch", "pytorch", "deep learning frameworks (e.g., pytorch, tensorflow)",
        "linear algebra (vectors, matrices)", "linear algebra",
        "Python", "python programming language",
        "NumPy", "SciPy",
    ]
    result = normalize_and_deduplicate_skills(test_input)
    print(f"Dedup test: {len(test_input)} raw â†’ {len(result)} unique")
    print(f"  Result: {result}")
    assert len(result) <= 4, f"Expected <= 4 unique, got {len(result)}: {result}"

    # èšåˆæµ‹è¯•
    mock_skills = [
        {
            "ml_methods": ["sparse autoencoder", "contrastive learning", "PyTorch"],
            "math_foundations": ["linear algebra", "convex optimization"],
            "programming_tools": ["PyTorch", "Python"],
            "domain_knowledge": ["recommender systems"],
            "interdisciplinary": [
                {"discipline": "Neuroscience", "specific_knowledge": "sparse coding in V1",
                 "why_needed": "Biological inspiration for sparse representation models"}
            ],
        },
        {
            "ml_methods": ["sparse autoencoder", "dictionary learning"],
            "math_foundations": ["linear algebra (vectors, matrices)", "matrix factorization"],
            "programming_tools": ["pytorch", "scikit-learn", "numpy"],
            "domain_knowledge": ["computer vision"],
            "interdisciplinary": [
                {"discipline": "neuroscience", "specific_knowledge": "visual cortex",
                 "why_needed": "Model motivation from biology"}
            ],
        },
        {
            "ml_methods": ["sparse autoencoder"],
            "math_foundations": ["linear algebra", "calculus (multivariate)"],
            "programming_tools": ["tensorflow", "scipy"],
            "domain_knowledge": ["NLP"],
            "interdisciplinary": [
                {"discipline": "Linguistics", "specific_knowledge": "syntax",
                 "why_needed": "Understanding language structure"}
            ],
        },
    ]

    agg = aggregate_skills(mock_skills)
    stats = agg["dedup_stats"]
    print(f"\nAggregation: {stats['before']} raw â†’ {stats['after']} unique")

    # Verify Neuroscience merged (appeared 2x), Linguistics filtered (1x)
    inter = agg["interdisciplinary_summary"]
    disc_names = [i["discipline"] for i in inter]
    print(f"Interdisciplinary (freq>=2): {disc_names}")
    assert "Neuroscience" in disc_names, "Neuroscience should appear (freq=2)"
    assert "Linguistics" not in disc_names, "Linguistics should be filtered (freq=1)"

    print(f"\nCategories: {_SKILL_CATEGORIES}")
    for tier in ["must_have", "important", "good_to_have"]:
        for cat in _SKILL_CATEGORIES:
            items = agg[tier][cat]
            if items:
                print(f"  {tier}/{cat}: {[i['skill'] for i in items]}")

    md = render_skills_markdown(agg)
    row_count = md.count("\n|") - 2  # subtract header + separator
    print(f"\nTable rows (excl header): {row_count} (max {_MAX_TABLE_ROWS})")
    print("\n" + md)
