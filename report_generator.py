# report_generator.py â€” Prompt 3ï¼šåŸºäºå‘é‡æ•°æ®åº“ç”Ÿæˆ Sparse Representation å¯¹æ¯”æŠ¥å‘Š

import json
import time
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI

from config import (
    CREDIBILITY_THRESHOLD,
    DAYS_BACK,
    DOMAIN,
    EMPTY_SECTION_PHRASES,
    GEMINI_MODEL,
    GEMINI_MODEL_FAST,
    LOW_CONFIDENCE_REPORT_MODE,
    MIN_PAPERS_FOR_COMPARISON,
    MIN_PAPERS_FOR_ROADMAP,
    MIN_PAPERS_FOR_TREND_ANALYSIS,
    REPORT_OUTPUT_PATH,
    THINKING_BUDGET,
)
from credibility_scorer import score_paper_credibility
from paper_extractor import load_db
from trend_analyzer import analyze_trends, render_trends_markdown

load_dotenv()


def _make_llm(deep: bool = False) -> ChatGoogleGenerativeAI:
    """
    LLM å·¥å‚å‡½æ•°ã€‚
    åŠ å…¥ max_retries=1 é˜²æ­¢ 429 æŠ¥é”™æ—¶æ— é™æŒ‚èµ·å¯¼è‡´ UI å¡æ­»ã€‚
    """
    if deep:
        kwargs = {
            "model": GEMINI_MODEL,
            "temperature": 1,
            "max_retries": 1,
        }
        if "thinking" in GEMINI_MODEL or "preview" in GEMINI_MODEL:
            kwargs["thinking_budget"] = THINKING_BUDGET
        return ChatGoogleGenerativeAI(**kwargs)

    return ChatGoogleGenerativeAI(
        model=GEMINI_MODEL_FAST,
        temperature=0.2,
        max_retries=1,
    )


# â”€â”€ Prompt 3ï¼šæŠ¥å‘Šç”Ÿæˆï¼ˆå«é˜²å¹»è§‰æŒ‡ä»¤ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

REPORT_SYSTEM_PROMPT = """You are a senior research analyst.
You will synthesize a set of recent academic paper summaries into a structured comparative report in Markdown.

ANTI-HALLUCINATION RULES (CRITICAL â€” follow these strictly):
1. Only make claims that are explicitly supported by the paper summaries provided below.
2. If you are uncertain about a fact, write "[needs verification]" instead of guessing.
3. Do not invent author names, publication dates, benchmark results, or dataset names.
4. When comparing papers, cite the paper title directly (e.g., "As shown in 'Title...', ...")
5. Do not merge findings from different papers without clearly attributing each.

Report structure (follow exactly, do not add or remove sections):

# Research Landscape: {domain} â€” Last {days} Days
*Generated: {date}*

## 1. Overview
[2-3 sentences on the overarching trend across all provided papers]

## 2. Key Themes & Breakthroughs
[Group papers by shared technical themes. For each theme:
 - Name the theme as a subheading (###)
 - Describe the core idea in 1-2 sentences
 - List which papers (by title) fall under this theme
 - Describe the advancement over prior work based only on what the summaries say
 - CRITICAL: If the advancement appears incremental rather than fundamental
   (e.g. marginal metric improvements, same approach on a new dataset, minor
   architectural variants), explicitly note this with phrasing like:
   "This represents an incremental advance - [specific reason]."
   Do not manufacture significance that is not supported by the evidence.]

## 4. Identified Gaps & Open Problems
[Reserved section. Do not analyze gaps here.
 Output only: "Section generated by extrapolated gaps module."]

## 5. Recommended Reading Order
[Recommend EXACTLY 3 to 5 papers maximum.
 Selection and ordering criteria (apply in this order):
 1. FIRST paper: highest domain_specificity score with theoretical_depth between 2-4.
    This gives the reader the clearest picture of the field without overwhelming formalism.
 2. MIDDLE papers: papers that represent methodologically distinct approaches
    (e.g. one experimental, one ML-based, one simulation-based if available).
    Prefer higher domain_specificity over lower theoretical_depth.
 3. LAST paper: highest theoretical_depth among core-domain papers, for readers
    who want the deepest technical treatment.
 For each paper include:
 - Title in bold only (no "Paper N:" prefix - the list number is sufficient)
 - First author only, followed by institution in parentheses if and only if
   the institution is explicitly present in the provided paper data.
   If institution data is absent or unclear, write only the author name.
   NEVER write "Affiliation not provided", "Unknown", or any placeholder.
   Format when institution known: "Author et al. (Institution)"
   Format when institution unknown: "Author et al."
 - One sentence of justification citing its unique contribution
 Never recommend papers with is_core_domain=false.
 Never recommend more than 5 papers regardless of the total paper count.]

## 6. Summary Table
[Output the markdown table with these columns:
 | Title (short) | First Author | Sub-domain | Method Type | Readiness/Testability | Theoretical Depth | Domain Specificity | Credibility | Key Contribution |
 COLUMN ROUTING: If the papers are primarily from theoretical physics,
 cosmology, astrophysics, or pure mathematics, the 5th column should be
 "Observational Testability" (1=untestable, 3=next-gen instruments, 5=current data).
 Otherwise use "Industrial Readiness" (1=theoretical, 3=real data, 5=production).
 Fill only from the data provided.
 Immediately BEFORE the first table row (after the header separator line),
 do NOT output a legend â€” the legend will be injected by the rendering code.
 Do NOT output any legend block. Just the table.]

IMPORTANT: Do NOT generate a Section 3 â€” it will be added separately."""

REPORT_USER_TEMPLATE = """Domain: {domain}
Date range: Last {days} days

Paper Summaries (JSON):
{paper_summaries}

Write the full report now."""


GAPS_SYSTEM_PROMPT = """You are an advanced research strategist.
Given a set of recent paper summaries, infer what remains unsolved AFTER considering
the combined solutions across all papers.

Reasoning requirements (strict):
1. First synthesize the core solutions proposed across all provided papers.
2. Then perform cross-paper reasoning to find blind spots:
   edge cases, systemic issues, and interdisciplinary challenges that remain unsolved
   even if all proposed solutions work perfectly.
3. DO NOT summarize the problems the authors were trying to solve.
   You must extrapolate and deduce NEW problems or overlooked areas that emerge
   from the papers' combined methodologies.
4. STRICT RULE: If you find yourself describing a problem that one input paper
   claims to solve, discard it and find a different gap.

Output ONLY valid JSON with this exact schema:
{
  "extrapolated_gaps": [
    {
      "gap_title": "string (< 10 words)",
      "description": "string (2-3 sentences, future-looking)",
      "affected_papers": ["title1", "title2"],
      "why_unsolved": "string (1 sentence)",
      "potential_research_direction": "string (1 sentence)"
    }
  ]
}

Rules:
- Output ONLY gaps that are genuinely supported by cross-paper reasoning.
  Do not manufacture gaps to meet a minimum count.
- Target 3-6 gaps for a normal paper set (5+ papers).
- For a small paper set (2-4 papers), 1-3 well-evidenced gaps is acceptable.
  A short list of real gaps is better than a long list of fabricated ones.
- Each gap must be logically distinct.
- PHYSICAL SCALE TEST (mandatory before accepting any cross-paper gap):
  Identify the characteristic physical scale of each affected paper
  (e.g., planetary atmosphere ~10^7 m, pulsar timing ~10^20 m, cosmological
  structure ~10^26 m, molecular bond ~10^-10 m). If the scales differ by
  more than ~6 orders of magnitude, the papers operate in fundamentally
  different physical regimes and cannot directly inform each other's gaps.
  Discard the gap. A valid gap must arise from papers that share the same
  physical regime, observational wavelength band, or mathematical framework.
- CAUSAL MECHANISM TEST: There must be a direct physical, mathematical, or
  experimental mechanism by which progress in Paper A would require or enable
  progress in Paper B. Shared surface-level vocabulary ("signal", "wave",
  "structure") is not a mechanism. If you cannot name the mechanism in one
  concrete sentence, discard the gap.
- COSMOLOGICAL EPOCH ISOLATION (mandatory for astrophysics / cosmology papers):
  Classify each paper into one of these epochs:
    (a) Planck / inflation / reheating (t < 10^-32 s)
    (b) Early-universe phase transitions / baryogenesis (10^-32 s < t < 1 s)
    (c) BBN / recombination / CMB (1 s < t < 380 kyr)
    (d) Dark ages / cosmic dawn / reionization (380 kyr < t < 1 Gyr)
    (e) Late-time structure formation / galaxy evolution (t > 1 Gyr)
    (f) Purely mathematical / formal theory (no specific epoch)
  A gap that links papers from epochs separated by more than 2 steps
  (e.g., epoch (a) with epoch (e)) is almost certainly a forced
  extrapolation. You MUST discard it UNLESS you can cite a concrete,
  non-trivial physical mechanism (not just shared terminology) that
  causally connects the two epochs within the specific models discussed
  in the papers. "Both involve gravity" or "both concern dark matter"
  is NOT sufficient â€” you must describe the actual coupling channel.
- No markdown fences, no explanations outside JSON."""

GAPS_DOMAIN_SPECIFICITY_RULE_TEMPLATE = """DOMAIN SPECIFICITY RULE (CRITICAL):
You are analyzing papers in the domain: {domain}

MANDATORY TERMINOLOGY TEST â€” before finalizing each gap, apply this check:
  Does this gap description contain AT LEAST TWO technical terms that are
  specific to {domain} and would NOT appear in a gap analysis for a completely
  different field (e.g. economics, NLP, or astrophysics)?

  If the answer is NO â€” the gap is too generic. Discard it and look deeper
  into the specific methodological tensions, material constraints, physical
  phenomena, or measurement challenges that appear in these exact papers.

Examples of domain-specific terms by field (for calibration only):
  - Anti-Ice / Surface Science â†’ contact angle, ice adhesion force, Cassie-Baxter state,
    nucleation kinetics, wettability, hierarchical texture, icephobicity
  - Astrophysics â†’ redshift, stellar population, spectral energy distribution,
    interferometric baseline, dark matter halo, cosmological parameter
  - NLP â†’ tokenization, perplexity, attention head, context window, hallucination rate
  - Computer Vision â†’ feature pyramid, anchor box, IoU threshold, backbone network

Your gaps must read as if written by a domain expert, not a generalist AI."""

GAPS_USER_TEMPLATE = """Paper Summaries (JSON):
{paper_summaries}

Generate extrapolated gaps now."""


def retrieve_all_papers_from_db() -> list[dict]:
    """ä»å‘é‡æ•°æ®åº“å–å‡ºæ‰€æœ‰å­˜å‚¨çš„è®ºæ–‡"""
    try:
        db = load_db()
        # Use the configured domain as broad retrieval query.
        search_query = (DOMAIN or "research papers").strip() or "research papers"
        results = db.similarity_search(search_query, k=50)
        papers = []
        seen_ids = set()
        for doc in results:
            meta = doc.metadata
            arxiv_id = meta.get("arxiv_id", "")
            if arxiv_id in seen_ids:
                continue
            seen_ids.add(arxiv_id)
            full = json.loads(meta.get("full_json", "{}"))
            if full:
                papers.append(full)
        print(f"[Report] ä»æ•°æ®åº“å–å‡º {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    except Exception as e:
        print(f"[Report] æ•°æ®åº“è¯»å–å¤±è´¥ï¼š{e}")
        return []


def generate_report(
    papers: list[dict],
    domain: str | None = None,
    days: int | None = None,
) -> str:
    """
    è°ƒç”¨ Gemini ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šã€‚
    """
    if not papers:
        return "é”™è¯¯ï¼šæ²¡æœ‰è®ºæ–‡æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ agent.pyã€‚"

    # ç²¾ç®€ JSONï¼Œåªä¿ç•™æŠ¥å‘Šéœ€è¦çš„å­—æ®µï¼ŒèŠ‚çœ token
    slim_papers = []
    for p in papers:
        slim_papers.append(
            {
                "title": p.get("title"),
                "published_date": p.get("published_date"),
                "sub_domain": p.get("sub_domain"),
                "problem_statement": p.get("problem_statement"),
                "method_summary": p.get("method_summary"),
                "contributions": p.get("contributions"),
                "limitations": p.get("limitations"),
                "method_keywords": p.get("method_keywords"),
                "method_type": (p.get("methodology_matrix") or {}).get("approach_type"),
                "open_source": (p.get("methodology_matrix") or {}).get("open_source"),
                "github_url": p.get("github_url") if p.get("github_url_validated") else None,
                "is_core_domain": p.get("is_core_domain", True),
                "relevance_score": p.get("relevance_score"),
                "industrial_readiness_score": p.get("industrial_readiness_score"),
                "theoretical_depth": p.get("theoretical_depth"),
                "domain_specificity": p.get(
                    "domain_specificity",
                    p.get("relevance_to_sparse_representation"),
                ),
                "url": p.get("url"),
            }
        )

    paper_summaries_str = json.dumps(slim_papers, ensure_ascii=False, indent=2)
    today = datetime.now().strftime("%Y-%m-%d")

    topic_domain = (domain or DOMAIN).strip() if isinstance(domain, str) else DOMAIN
    if not topic_domain:
        topic_domain = DOMAIN

    report_days = DAYS_BACK
    try:
        if days is not None:
            report_days = int(days)
    except (TypeError, ValueError):
        report_days = DAYS_BACK
    if report_days <= 0:
        report_days = DAYS_BACK

    system_prompt = REPORT_SYSTEM_PROMPT.format(
        domain=topic_domain,
        days=report_days,
        date=today,
    )
    user_prompt = REPORT_USER_TEMPLATE.format(
        domain=topic_domain,
        days=report_days,
        paper_summaries=paper_summaries_str,
    )

    llm = _make_llm(deep=False)  # fast model â€” enough for structured report generation
    print(f"[Report] æ­£åœ¨ç”ŸæˆæŠ¥å‘Š ({GEMINI_MODEL_FAST})ï¼Œè¯·ç¨å€™...")

    response = llm.invoke(
        [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt),
        ]
    )
    return response.content


def _clean_json_response(raw: str) -> str:
    raw = raw.strip()
    if raw.startswith("```"):
        raw = raw.split("```")[1]
        if raw.startswith("json"):
            raw = raw[4:]
    return raw.strip()


def _text_blob(value) -> str:
    """Normalize mixed string/list content into one lowercase text blob."""
    if isinstance(value, list):
        return " ".join(str(v) for v in value if v is not None).lower()
    if value is None:
        return ""
    return str(value).lower()


def _collect_gap_terms(papers: list[dict]) -> tuple[list[str], list[str], list[str]]:
    all_keywords: list[str] = []
    all_subdomains: list[str] = []
    all_methods: list[str] = []
    for paper in papers[:5]:
        method_keywords = paper.get("method_keywords")
        if isinstance(method_keywords, list):
            all_keywords.extend(str(kw) for kw in method_keywords if kw)

        sub_domain = str(paper.get("sub_domain", "")).strip()
        if sub_domain:
            all_subdomains.append(sub_domain)

        methodology_matrix = paper.get("methodology_matrix")
        approach = ""
        if isinstance(methodology_matrix, dict):
            approach = str(methodology_matrix.get("approach_type", "")).strip()
        if approach and approach not in {"none (theoretical)", "â€”"}:
            all_methods.append(approach)

    return all_keywords, all_subdomains, all_methods


def _derive_gap_context(papers: list[dict], domain: str) -> dict:
    domain_name = str(domain).strip() if domain else ""
    if not domain_name:
        domain_name = DOMAIN
    if not papers:
        return {
            "domain_name": domain_name,
            "keyword_str": "existing techniques",
            "subdomain_str": domain_name,
            "method_str": "current methods",
            "paper_titles": [],
        }

    # Derive real vocabulary from actual papers - never use ML templates.
    all_keywords, all_subdomains, all_methods = _collect_gap_terms(papers)

    keyword_str = ", ".join(dict.fromkeys(all_keywords[:6])) or "existing techniques"
    subdomain_str = " and ".join(dict.fromkeys(all_subdomains[:2])) or domain_name
    method_str = " and ".join(dict.fromkeys(all_methods[:3])) or "current methods"
    paper_titles = [str(p.get("title") or "Untitled").strip() for p in papers[:3]]

    return {
        "domain_name": domain_name,
        "keyword_str": keyword_str,
        "subdomain_str": subdomain_str,
        "method_str": method_str,
        "paper_titles": paper_titles,
    }


def _build_fallback_gap_items(context: dict) -> list[dict]:
    domain_name = context["domain_name"]
    keyword_str = context["keyword_str"]
    subdomain_str = context["subdomain_str"]
    method_str = context["method_str"]
    paper_titles = context["paper_titles"]
    return [
        {
            "gap_title": f"Multi-condition validation gap in {domain_name}",
            "description": (
                f"Studies in {subdomain_str} test performance under isolated lab conditions, "
                "but real-world deployment combines multiple simultaneous stressors "
                f"(e.g. humidity, thermal cycling, mechanical stress) not addressed by {keyword_str}. "
                "No paper evaluates combined-stress scenarios."
            ),
            "affected_papers": paper_titles,
            "why_unsolved": (
                "Replicating multi-variable field conditions requires test infrastructure "
                "that exceeds typical lab-scale experimental setups."
            ),
            "potential_research_direction": (
                f"Develop standardized combined-stress test protocols for {domain_name} "
                "that reflect real deployment environments rather than idealized lab conditions."
            ),
        },
        {
            "gap_title": f"Long-term durability characterization in {domain_name}",
            "description": (
                f"Current {domain_name} papers report initial performance metrics using {method_str}, "
                "but systematic long-term aging studies (months to years under cyclic conditions) "
                "are absent from the literature surveyed."
            ),
            "affected_papers": paper_titles,
            "why_unsolved": (
                "Long-term studies require sustained experimental resources and fall outside "
                "typical academic publication timelines."
            ),
            "potential_research_direction": (
                f"Establish accelerated aging protocols for {domain_name} that correlate "
                "with real-world lifetime, enabling faster durability screening."
            ),
        },
        {
            "gap_title": f"Cross-study reproducibility in {domain_name}",
            "description": (
                f"The papers in {subdomain_str} use different measurement conditions, sample preparation "
                "procedures, and reporting units, making direct quantitative comparison of results impossible. "
                "Findings from one study cannot be reliably transferred to another experimental setup."
            ),
            "affected_papers": paper_titles,
            "why_unsolved": (
                f"No shared measurement standard or reporting protocol exists that is "
                f"universally adopted across {domain_name} research groups."
            ),
            "potential_research_direction": (
                f"Propose a community-agreed minimum reporting standard for {domain_name} "
                "experiments, specifying sample geometry, environmental conditions, and key metrics."
            ),
        },
    ]


def _fallback_extrapolated_gaps(papers: list[dict], domain: str) -> dict:
    if not papers:
        return {"extrapolated_gaps": []}
    context = _derive_gap_context(papers, domain)
    return {"extrapolated_gaps": _build_fallback_gap_items(context)}


def generate_extrapolated_gaps(
    papers: list[dict],
    llm: ChatGoogleGenerativeAI,
    domain: str | None = None,
) -> dict:
    """
    Generate Section-4 gap analysis via dedicated cross-paper reasoning prompt.
    """
    slim_papers = []
    for p in papers:
        slim_papers.append(
            {
                "title": p.get("title"),
                "sub_domain": p.get("sub_domain"),
                "problem_statement": p.get("problem_statement"),
                "method_summary": p.get("method_summary"),
                "contributions": p.get("contributions"),
                "methodology_matrix": p.get("methodology_matrix"),
            }
        )

    user_prompt = GAPS_USER_TEMPLATE.format(
        paper_summaries=json.dumps(slim_papers, ensure_ascii=False, indent=2)
    )

    domain_name = (domain or DOMAIN).strip() if isinstance(domain, str) else DOMAIN
    if not domain_name:
        domain_name = DOMAIN

    domain_rule = GAPS_DOMAIN_SPECIFICITY_RULE_TEMPLATE.format(domain=domain_name)
    system_prompt = GAPS_SYSTEM_PROMPT + "\n\n" + domain_rule

    try:
        response = llm.invoke(
            [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt),
            ]
        )
        parsed = json.loads(_clean_json_response(response.content))
        raw_gaps = parsed.get("extrapolated_gaps", [])
        if not isinstance(raw_gaps, list):
            raw_gaps = []

        normalized = []
        seen_titles = set()
        for gap in raw_gaps:
            if not isinstance(gap, dict):
                continue
            gap_title = str(gap.get("gap_title", "")).strip()
            if not gap_title:
                continue
            key = gap_title.lower()
            if key in seen_titles:
                continue
            seen_titles.add(key)

            affected = gap.get("affected_papers", [])
            if not isinstance(affected, list):
                affected = [str(affected)]
            affected = [str(t).strip() for t in affected if str(t).strip()]

            normalized.append(
                {
                    "gap_title": gap_title,
                    "description": str(gap.get("description", "")).strip(),
                    "affected_papers": affected,
                    "why_unsolved": str(gap.get("why_unsolved", "")).strip(),
                    "potential_research_direction": str(
                        gap.get("potential_research_direction", "")
                    ).strip(),
                }
            )

        normalized = normalized[:6]
        if normalized:
            return {"extrapolated_gaps": normalized}
        return _fallback_extrapolated_gaps(papers, domain_name)
    except Exception as e:
        print(f"[Report] extrapolated gaps generation failed: {e}")
        return _fallback_extrapolated_gaps(papers, domain_name)


def render_extrapolated_gaps_markdown(gap_data: dict) -> str:
    """
    Render dedicated extrapolated gaps JSON into Section 4 markdown.
    """
    lines = [
        "## 4. Identified Gaps & Open Problems",
        "",
    ]
    gaps = gap_data.get("extrapolated_gaps", [])
    if not isinstance(gaps, list):
        gaps = []

    if not gaps:
        lines.append("[needs verification] Extrapolated cross-paper gaps were not generated.")
        return "\n".join(lines)

    for g in gaps:
        title = g.get("gap_title") or "Untitled gap"
        desc = g.get("description") or "[needs verification]"
        affected = g.get("affected_papers") or []
        why = g.get("why_unsolved") or "[needs verification]"
        direction = g.get("potential_research_direction") or "[needs verification]"

        affected_text = ", ".join(affected) if affected else "[needs verification]"

        lines.append(f"### {title}")
        lines.append(desc)
        lines.append(f"- Affected papers: {affected_text}")
        lines.append(f"- Why unsolved: {why}")
        lines.append(f"- Potential research direction: {direction}")
        lines.append("")

    return "\n".join(lines).strip()


def inject_section_four(report_text: str, section_text: str) -> str:
    """
    Replace Section 4 with supplied markdown section.
    If missing, insert before Section 5 or append at end.
    """
    import re

    sec4_pattern = re.compile(r"## 4\..*?(?=\n## |\Z)", re.DOTALL)
    if sec4_pattern.search(report_text):
        return sec4_pattern.sub(lambda _m: section_text, report_text)

    sec5_pattern = re.compile(r"(?=^## 5\.)", re.MULTILINE)
    if sec5_pattern.search(report_text):
        insert_text = section_text + "\n\n"
        return sec5_pattern.sub(lambda _m: insert_text, report_text, count=1)

    return report_text.rstrip() + "\n\n" + section_text + "\n"


def clean_markdown_tables(text: str) -> str:
    """
    ä¿®å¤ Gemini ç”Ÿæˆçš„ Markdown è¡¨æ ¼é—®é¢˜ï¼š
    1. å°†è¿‡é•¿çš„åˆ†éš”è¡Œ | :---...--- | è§„èŒƒåŒ–ä¸º | --- |
    2. å»æ‰çº¯ç©ºç™½çš„è¶…é•¿è¡Œï¼ˆLLM æœ‰æ—¶ä¼šè¾“å‡ºæ•°åä¸‡ç©ºæ ¼ä»£æ›¿è¡¨æ ¼è¡Œï¼‰
    3. ç¡®ä¿åˆ†éš”è¡Œåˆ—æ•°ä¸ä¸Šæ–¹è¡¨å¤´è¡Œçš„åˆ—æ•°ä¸€è‡´
    """
    import re

    sep_pattern = re.compile(r"^(\|[ \t]*:?-{1,}:?[ \t]*)+\|?[ \t]*$")
    cleaned_lines = []
    for line in text.splitlines():
        # è·³è¿‡çº¯ç©ºç™½ä¸”å¼‚å¸¸é•¿çš„è¡Œï¼ˆ> 200 å­—ç¬¦çš„çº¯ç©ºç™½ï¼‰
        if len(line) > 200 and line.strip() == "":
            continue
        if sep_pattern.match(line.strip()):
            # ä»ä¸Šä¸€è¡Œï¼ˆè¡¨å¤´è¡Œï¼‰æ¨æ–­åˆ—æ•°
            col_count = None
            if cleaned_lines:
                header = cleaned_lines[-1]
                if "|" in header:
                    cols = [c for c in header.split("|") if c.strip()]
                    col_count = len(cols)
            if col_count is None or col_count < 1:
                col_count = max(line.count("|") - 1, 1)
            cleaned_lines.append("| " + " | ".join(["---"] * col_count) + " |")
        else:
            cleaned_lines.append(line)
    return "\n".join(cleaned_lines)


# â”€â”€ æ™ºèƒ½ç« èŠ‚éšè— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def should_render_section(content) -> bool:
    """
    å†…å®¹é—¨æ§ï¼šè¿”å› False è¡¨ç¤ºè¯¥ç« èŠ‚åº”å®Œå…¨éšè—ã€‚
    content å¯ä»¥æ˜¯ None / ç©ºåˆ—è¡¨ / å­—ç¬¦ä¸²ã€‚
    """
    if content is None:
        return False
    if isinstance(content, list):
        return len(content) > 0
    if isinstance(content, str):
        if not content.strip():
            return False
        content_lower = content.lower()
        return not any(phrase in content_lower for phrase in EMPTY_SECTION_PHRASES)
    return True


def strip_empty_sections(report_text: str) -> str:
    """
    å¯¹ Gemini ç”Ÿæˆçš„æŠ¥å‘Šè¿›è¡Œåå¤„ç†ï¼š
    ç§»é™¤å†…å®¹ä¸­åŒ…å« EMPTY_SECTION_PHRASES çš„ Section 4ï¼ˆGapsï¼‰å—ã€‚
    å°†æ–‡æ¡£æŒ‰ ## çº§æ ‡é¢˜æ‹†åˆ†ï¼Œå¯¹ ## 4. è¿›è¡Œé—¨æ§ï¼Œå†é‡æ–°æ‹¼æ¥ã€‚
    """
    import re

    section_splitter = re.compile(r"(?=^## )", re.MULTILINE)
    chunks = section_splitter.split(report_text)

    result = []
    for chunk in chunks:
        if chunk.startswith("## 4."):
            # æå–æ­£æ–‡ï¼ˆæ ‡é¢˜è¡Œä¹‹åï¼‰
            body_start = chunk.find("\n")
            body = chunk[body_start:] if body_start != -1 else ""
            if not should_render_section(body):
                continue  # å®Œå…¨ä¸¢å¼ƒè¿™ä¸ªå—
        result.append(chunk)

    return "".join(result)


# â”€â”€ é˜…è¯»é¡ºåºå¢å¼ºï¼šæ³¨å…¥ arXiv é“¾æ¥ + BibTeX â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _build_bibtex(paper: dict) -> str:
    """æ ¹æ®è®ºæ–‡å…ƒæ•°æ®ç”Ÿæˆ BibTeX æ¡ç›®"""
    arxiv_id = (paper.get("arxiv_id") or "unknown").replace("/", "_")
    title = paper.get("title", "Unknown Title")
    authors = paper.get("authors") or []
    first_author = authors[0].split()[-1] if authors else "Unknown"
    year = (paper.get("published_date") or "0000")[:4]
    url = paper.get("url", "")
    return (
        f"@article{{arxiv_{arxiv_id},\n"
        f"  title={{{title}}},\n"
        f"  author={{{first_author} et al.}},\n"
        f"  year={{{year}}},\n"
        f"  url={{{url}}}\n"
        f"}}"
    )


def _strip_paper_n_labels(text: str) -> str:
    """
    Remove 'Paper N:' or 'Paper N -' prefixes inside bold spans.
    Example: '**Paper 3: Some Title**' -> '**Some Title**'.
    """
    import re

    return re.sub(
        r"(\*\*)Paper\s+\d+[:\-\u2013\u2014]?\s*",
        r"\1",
        text,
        flags=re.IGNORECASE,
    )


def _strip_affiliation_placeholders(text: str) -> str:
    """
    Remove LLM-generated affiliation placeholders from user-facing output.
    """
    import re

    patterns = [
        r",?\s*\(Affiliation not provided\)",
        r",?\s*\(affiliation not provided\)",
        r",?\s*\(Affiliation unknown\)",
        r",?\s*\(affiliation unknown\)",
        r",?\s*\(Unknown\)",
        r",?\s*Affiliation not provided",
        r",?\s*affiliation not provided",
    ]
    for pattern in patterns:
        text = re.sub(pattern, "", text, flags=re.IGNORECASE)
    return text


def enhance_reading_order(report_text: str, papers: list[dict]) -> str:
    """
    åœ¨ Section 5 (Recommended Reading Order) ä¸­ï¼Œä¸ºæ¯ç¯‡è®ºæ–‡æ³¨å…¥ï¼š
    - arXiv é“¾æ¥
    - å¯æŠ˜å  BibTeX å—

    ç­–ç•¥ï¼š
    1. æå– "## 5." ç« èŠ‚å†…å®¹
    2. æŒ‰è¡Œæ‰«æï¼Œè¯†åˆ«ä»¥æ•°å­—+ç‚¹æˆ– ** å¼€å¤´çš„æ¡ç›®æ ‡é¢˜
    3. åœ¨æ¯ä¸ªæ¡ç›®æ ‡é¢˜è¡Œä¸‹æ’å…¥é“¾æ¥ + BibTeXï¼ˆé€šè¿‡æ ‡é¢˜æ¨¡ç³ŠåŒ¹é…è®ºæ–‡ï¼‰
    4. æ›¿æ¢å›åŸæ–‡æ¡£
    """
    import re
    from difflib import SequenceMatcher

    # æ„å»ºæ ‡é¢˜ â†’ è®ºæ–‡çš„æŸ¥æ‰¾ç»“æ„ï¼ˆå°å†™ï¼‰
    title_to_paper = {p.get("title", "").lower().strip(): p for p in papers if p.get("title")}

    def _find_paper(mention: str) -> dict | None:
        mention_lower = mention.lower().strip()
        if mention_lower in title_to_paper:
            return title_to_paper[mention_lower]
        for title, paper in title_to_paper.items():
            if mention_lower in title or title in mention_lower:
                return paper
        best, best_ratio = None, 0.0
        for title, paper in title_to_paper.items():
            ratio = SequenceMatcher(None, mention_lower, title).ratio()
            if ratio > best_ratio:
                best_ratio, best = ratio, paper
        return best if best_ratio >= 0.6 else None

    # æ‰¾åˆ° Section 5 çš„èŒƒå›´ï¼ˆç›´åˆ°ä¸‹ä¸€ä¸ª ## æ ‡é¢˜æˆ–æ–‡æ¡£ç»“å°¾ï¼‰
    sec5_match = re.search(r"(## 5\..*?)(?=\n## |\Z)", report_text, re.DOTALL)
    if not sec5_match:
        return report_text

    sec5_original = sec5_match.group(0)
    sec5_original = _strip_paper_n_labels(sec5_original)
    sec5_original = _strip_affiliation_placeholders(sec5_original)
    sec5_lines = sec5_original.splitlines(keepends=True)

    # è¯†åˆ«æ¡ç›®è¡Œï¼šä»¥ "N. " æˆ– "**Title**" å¼€å¤´çš„è¡Œ
    entry_pattern = re.compile(r"^(\d+\.\s+|\*\*)(.+?)(\*\*)?\s*$")
    reading_order_items = [
        idx for idx, line in enumerate(sec5_lines) if entry_pattern.match(line.strip())
    ]
    reading_order_items = reading_order_items[:5]  # enforce cap regardless of LLM output

    if reading_order_items:
        trimmed_sec5_lines = sec5_lines[: reading_order_items[0]]
        for item_pos, start_idx in enumerate(reading_order_items):
            end_idx = (
                reading_order_items[item_pos + 1]
                if item_pos + 1 < len(reading_order_items)
                else len(sec5_lines)
            )
            block = list(sec5_lines[start_idx:end_idx])
            if block and re.match(r"^\d+\.\s+", block[0].strip()):
                block[0] = re.sub(r"^\d+\.", f"{item_pos + 1}.", block[0], count=1)
            trimmed_sec5_lines.extend(block)
        sec5_lines = trimmed_sec5_lines

    enhanced_lines = []

    for line in sec5_lines:
        enhanced_lines.append(line)
        stripped = line.strip()
        m = entry_pattern.match(stripped)
        if m:
            candidate_title = m.group(2).strip().strip("*").strip()
            paper = _find_paper(candidate_title)
            if paper and paper.get("url"):
                url = paper["url"]
                bibtex = _build_bibtex(paper)
                enhanced_lines.append(
                    f"ğŸ”— [Read on arXiv]({url})  \n"
                    f"<details><summary>ğŸ“‹ BibTeX</summary>\n\n"
                    f"```bibtex\n{bibtex}\n```\n\n</details>\n\n"
                )

    sec5_enhanced = "".join(enhanced_lines)
    return report_text.replace(sec5_original, sec5_enhanced)


_EMPTY_CELL_STRINGS = frozenset({"none", "null", "n/a", "na", "unknown", "-", ""})


def _sanitize_md_cell(value) -> str:
    """Normalize markdown table cell content to keep table structure valid."""
    text = str(value if value is not None else "â€”")
    text = text.replace("\n", " ").replace("\r", " ").replace("|", "\\|").strip()
    if text.lower() in _EMPTY_CELL_STRINGS:
        return "â€”"
    return text or "â€”"


def _build_author_cell(paper: dict) -> str:
    import re

    authors = paper.get("authors") or []
    if not authors:
        return "â€”"

    first_author = str(authors[0]).strip()
    affiliations = str(paper.get("affiliations") or "").strip()
    if not affiliations:
        return first_author or "â€”"

    institution = re.split(r"[,;]", affiliations)[0].strip()[:40]
    return f"{first_author} ({institution})" if institution else first_author


def _build_credibility_cell(paper: dict) -> str:
    credibility = paper.get("credibility_score")
    if credibility is None:
        return "â€”"
    venue = paper.get("venue_detected")
    if venue:
        return f"{credibility}/100 ({venue})"
    return f"{credibility}/100"


def _first_contribution(paper: dict) -> str:
    contributions = paper.get("contributions")
    if isinstance(contributions, list) and contributions:
        return contributions[0]
    return "â€”"


# â”€â”€ Dynamic Schema Routing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Detect whether the paper set belongs to a pure-theory domain where
# "Industrial Readiness" is meaningless. When it is, swap the column for a
# domain-appropriate metric.

_THEORY_DOMAIN_KEYWORDS = frozenset({
    "cosmology", "astrophysics", "theoretical physics", "particle physics",
    "quantum gravity", "string theory", "high energy physics", "dark matter",
    "dark energy", "general relativity", "mathematical physics", "hep-th",
    "hep-ph", "gr-qc", "astro-ph", "nuclear theory",
})


def _detect_theory_dominant_domain(papers: list[dict]) -> bool:
    """Return True if >=60% of papers belong to a pure-theory domain."""
    if not papers:
        return False
    theory_count = 0
    for p in papers:
        sub_domain = str(p.get("sub_domain") or "").lower()
        primary_cat = str(p.get("primary_category") or "").lower()
        cats = [str(c).lower() for c in (p.get("categories") or [])]
        combined = f"{sub_domain} {primary_cat} {' '.join(cats)}"
        mm = p.get("methodology_matrix") or {}
        approach = str(mm.get("approach_type") or "").lower()
        industrial = p.get("industrial_readiness_score")
        # A paper is "pure theory" if its sub-domain or categories match, OR
        # if its approach is "theoretical" and industrial readiness <= 1.
        if any(kw in combined for kw in _THEORY_DOMAIN_KEYWORDS):
            theory_count += 1
        elif approach == "theoretical" and (industrial is None or industrial <= 1):
            theory_count += 1
    return theory_count / len(papers) >= 0.6


def _readiness_column_meta(papers: list[dict]) -> tuple[str, str, str]:
    """
    Return (column_header, legend_entry, score_field) based on domain.
    For theory-dominant domains: Observational Testability.
    Otherwise: Industrial Readiness.
    """
    if _detect_theory_dominant_domain(papers):
        return (
            "Observational Testability",
            "- **Observational Testability**: 1 = no conceivable observation "
            "can test this Â· 3 = testable with next-gen instruments Â· "
            "5 = already constrained by current data",
            "industrial_readiness_score",  # reuse same field; semantic swapped
        )
    return (
        "Industrial Readiness",
        "- **Industrial Readiness**: 1 = purely theoretical Â· 3 = tested on real "
        "materials or real-world data Â· 5 = pilot-scale / production validated",
        "industrial_readiness_score",
    )


def _scoring_legend(readiness_legend_line: str) -> str:
    return f"""\

---
**Scoring Legend** (all scores are on a 1â€“5 scale unless noted):
{readiness_legend_line}
- **Theoretical Depth**: 1 = engineering recipe with no derivation Â· 3 = derives \
key equations Â· 5 = rigorous proofs with generalization bounds
- **Domain Specificity**: 1 = domain is incidental context Â· 3 = methods \
designed for domain problems Â· 5 = primary contribution advances the domain
- **Credibility** (0â€“100): venue 40 Â· institution 10 Â· category match 15 Â· \
recency 20 Â· abstract richness 15
---"""


def render_summary_table(papers: list[dict]) -> str:
    """Render Section 6 summary table from structured paper data."""
    col_header, legend_line, score_field = _readiness_column_meta(papers)

    lines = [
        "## 6. Summary Table",
        "",
        f"| Title (short) | First Author | Sub-domain | Method Type | {col_header} | Theoretical Depth | Domain Specificity | Credibility | Key Contribution |",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- |",
    ]

    for p in papers:
        title = p.get("title") or "Untitled"
        title_short = title if len(title) <= 60 else f"{title[:57]}..."
        mm = p.get("methodology_matrix") or {}
        sub_domain = p.get("sub_domain") or "â€”"
        method_type = mm.get("approach_type") or "â€”"
        industrial = p.get(score_field)
        theory = p.get("theoretical_depth")
        specificity = p.get("domain_specificity", p.get("relevance_to_sparse_representation"))
        industrial = industrial if industrial not in (None, "") else "â€”"
        theory = theory if theory not in (None, "") else "â€”"
        specificity = specificity if specificity not in (None, "") else "â€”"
        author_cell = _build_author_cell(p)
        cred_cell = _build_credibility_cell(p)
        key_contribution = _first_contribution(p)

        lines.append(
            f"| {_sanitize_md_cell(title_short)} | {_sanitize_md_cell(author_cell)} "
            f"| {_sanitize_md_cell(sub_domain)} "
            f"| {_sanitize_md_cell(method_type)} | {_sanitize_md_cell(industrial)} "
            f"| {_sanitize_md_cell(theory)} | {_sanitize_md_cell(specificity)} "
            f"| {_sanitize_md_cell(cred_cell)} "
            f"| {_sanitize_md_cell(key_contribution)} |"
        )

    return "\n".join(lines) + _scoring_legend(legend_line)


def inject_summary_table(report_text: str, papers: list[dict]) -> str:
    """
    Replace Section 6 with a deterministic summary table built from paper data.
    If Section 6 is missing, append it at the end.
    """
    import re

    table_section = render_summary_table(papers)
    sec6_pattern = re.compile(r"## 6\. Summary Table.*?(?=\n## |\Z)", re.DOTALL)
    if sec6_pattern.search(report_text):
        return sec6_pattern.sub(lambda _m: table_section, report_text)
    return report_text.rstrip() + "\n\n" + table_section + "\n"


def _strip_system_instructions(text: str) -> str:
    """Remove leaked system-level prompt instructions from generated output.

    Catches patterns like:
      [System Instruction: Generate Section 3 here.]
      [SYSTEM: ...]
    Also removes any duplicate trailing section headings that were generated
    after the Summary Table (Section 6) â€” these are byproducts of the LLM
    re-generating content that the pipeline was supposed to inject.
    """
    import re

    # 1. Strip any [System Instruction: ...] / [SYSTEM: ...] tags
    text = re.sub(
        r"\[(?:System Instruction|SYSTEM|system instruction|System)\s*:.*?\]",
        "",
        text,
        flags=re.DOTALL,
    )

    # 2. Remove duplicate sections that appear AFTER Section 6.
    #    The canonical section order is 1-2-3-4-5-6.  Anything after
    #    Section 6 that repeats an earlier heading is leaked output.
    sec6_match = re.search(r"(## 6\. .+?)(?=\n## |\Z)", text, re.DOTALL)
    if sec6_match:
        after_sec6_start = sec6_match.end()
        before = text[:after_sec6_start]
        after = text[after_sec6_start:]
        # Remove any ## N. headings in the trailing section
        after = re.sub(r"\n## \d+\..*?(?=\n## |\Z)", "", after, flags=re.DOTALL)
        text = before + after

    # 3. Strip trailing whitespace / empty lines left over
    text = text.rstrip() + "\n"
    return text


def save_report(report_text: str, papers: list[dict] | None = None) -> str:
    """ä¿å­˜æŠ¥å‘Šåˆ° Markdown æ–‡ä»¶ï¼ˆå«åå¤„ç†ï¼šè¡¨æ ¼ä¿®å¤ + ç©ºç« èŠ‚éšè— + é“¾æ¥æ³¨å…¥ï¼‰

    Returns the fully post-processed report text so callers can keep
    in-memory state consistent with what was written to disk.
    """
    # --- Critical: strip leaked system instructions FIRST ---
    report_text = _strip_system_instructions(report_text)
    if papers:
        report_text = inject_summary_table(report_text, papers)
    report_text = clean_markdown_tables(report_text)
    report_text = strip_empty_sections(report_text)
    if papers:
        report_text = enhance_reading_order(report_text, papers)
    with open(REPORT_OUTPUT_PATH, "w", encoding="utf-8") as f:
        f.write(report_text)
    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{REPORT_OUTPUT_PATH}")
    return report_text


METHODOLOGY_CONTRAST_PROMPT = """You are given a methodology comparison matrix of research papers.
Write exactly 3-5 sentences that highlight the CONTRASTS between the papers.
Focus on: what makes each approach fundamentally different from the others.
Do NOT summarize individual papers. Only discuss differences.
Do NOT invent any information not present in the matrix."""


def _restore_truncated_titles(matrix_text: str, papers: list[dict]) -> str:
    """
    Scan a markdown table for truncated paper titles and restore them.
    A cell is considered truncated if it ends with '...' and matches the
    beginning of a known title.
    """
    import re

    known_titles = [str(p.get("title", "")) for p in papers if p.get("title")]

    def _restore_cell(match: re.Match[str]) -> str:
        cell = match.group(0).rstrip()
        if not cell.endswith("..."):
            return match.group(0)
        prefix = cell[:-3].strip()
        for title in known_titles:
            if title.startswith(prefix) and len(title) > len(prefix):
                return " " + _sanitize_md_cell(title) + " "
        return match.group(0)

    return re.sub(r"(?<=\|)[^|]{10,}?\.\.\.(?=\s*\|)", _restore_cell, matrix_text)


def render_methodology_matrix(papers: list[dict], llm=None) -> str:
    """
    ä»è®ºæ–‡æ•°æ®ä¸­æ„å»ºæ–¹æ³•è®ºå¯¹æ¯”çŸ©é˜µè¡¨æ ¼ + Gemini å·®å¼‚æ€§æ‘˜è¦ã€‚
    """
    lines = [
        "## 3. Methodology Comparison Matrix",
        "",
        "| Paper | Approach | Architecture | Supervision | Modality | Open Source | Theory | Key Baseline |",
        "| --- | --- | --- | --- | --- | --- | --- | --- |",
    ]

    has_missing = False
    matrix_rows_for_llm = []  # ç”¨äºç”Ÿæˆå¯¹æ¯”æ®µè½

    for p in papers:
        title = p.get("title") or "Untitled"
        title_cell = _sanitize_md_cell(title)
        mm = p.get("methodology_matrix")
        if not mm or not isinstance(mm, dict):
            lines.append(f"| {title_cell} | â€” | â€” | â€” | â€” | â€” | â€” | â€” |")
            has_missing = True
            continue

        approach = mm.get("approach_type", "â€”")
        arch = mm.get("model_architecture") or "â€”"
        supervision = mm.get("supervision_type", "â€”")
        modality = ", ".join(mm.get("data_modality", [])) or "â€”"
        oss = str(mm.get("open_source", "unknown")).strip().lower()
        github_url = p.get("github_url")
        github_url_validated = bool(p.get("github_url_validated"))
        if oss == "yes" and github_url_validated and github_url:
            open_source_display = f"âœ… [Code]({github_url})"
        elif oss == "yes":
            open_source_display = "âœ… Available (link unverified)"
        elif oss == "no":
            open_source_display = "âŒ Not available"
        else:
            open_source_display = "â€”"
        theory = mm.get("theoretical_guarantees", "â€”")
        baseline = mm.get("key_baseline_compared_to") or "â€”"

        lines.append(
            f"| {title_cell} | {_sanitize_md_cell(approach)} | {_sanitize_md_cell(arch)} "
            f"| {_sanitize_md_cell(supervision)} | {_sanitize_md_cell(modality)} "
            f"| {_sanitize_md_cell(open_source_display)} | {_sanitize_md_cell(theory)} "
            f"| {_sanitize_md_cell(baseline)} |"
        )
        matrix_rows_for_llm.append(
            {
                "title": title,
                "approach": approach,
                "architecture": arch,
                "supervision": supervision,
                "modality": modality,
                "theory": theory,
            }
        )

    lines.append("")

    if has_missing:
        lines.append(
            "> â„¹ï¸ Some papers lack methodology matrix data. "
            "Re-run `python agent.py` to extract it for all papers."
        )
        lines.append("")

    # ç”Ÿæˆå·®å¼‚æ€§æ‘˜è¦ï¼ˆ3â€“5 å¥ï¼‰
    if llm and len(matrix_rows_for_llm) >= 2:
        matrix_json = json.dumps(matrix_rows_for_llm, ensure_ascii=False, indent=2)
        try:
            print("[Report] æ­£åœ¨ç”Ÿæˆæ–¹æ³•è®ºå¯¹æ¯”æ‘˜è¦...")
            resp = llm.invoke(
                [
                    SystemMessage(content=METHODOLOGY_CONTRAST_PROMPT),
                    HumanMessage(content=f"Methodology matrix:\n{matrix_json}"),
                ]
            )
            contrast = resp.content.strip()
            lines.append(contrast)
            lines.append("")
        except Exception as e:
            print(f"[Report] å¯¹æ¯”æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{e}")

    raw = "\n".join(lines)
    return _restore_truncated_titles(raw, papers)


def _confidence_level(paper_count: int) -> str:
    """æ ¹æ®è®ºæ–‡æ•°é‡è¿”å›ç½®ä¿¡åº¦ç­‰çº§å­—ç¬¦ä¸²"""
    if paper_count < MIN_PAPERS_FOR_TREND_ANALYSIS:
        return "LOW"
    if paper_count < MIN_PAPERS_FOR_ROADMAP:
        return "MEDIUM"
    return "HIGH"


def render_single_paper_summary_cards(papers: list[dict]) -> str:
    """
    Low-confidence fallback: render only single-paper summary cards.
    No cross-paper sections should be included.
    """
    blocks = []
    for p in papers:
        title = p.get("title") or "Untitled"
        authors = ", ".join(p.get("authors", []) or [])
        published_date = p.get("published_date") or "unknown"
        url = p.get("url") or ""
        sub_domain = p.get("sub_domain") or "unknown"
        problem_statement = p.get("problem_statement") or "N/A"
        method_summary = p.get("method_summary") or "N/A"
        contributions = p.get("contributions") or []
        if not isinstance(contributions, list):
            contributions = [str(contributions)]
        if not contributions:
            contributions = ["N/A"]

        contribution_lines = "\n".join(
            f"- {str(item).strip()}" for item in contributions if str(item).strip()
        )
        if not contribution_lines:
            contribution_lines = "- N/A"

        blocks.append(
            "\n".join(
                [
                    "---",
                    f"### {title}",
                    f"**Authors:** {authors} | **Date:** {published_date} | \U0001f517 [arXiv]({url})",
                    f"**Sub-domain:** {sub_domain}",
                    f"**Problem:** {problem_statement}",
                    f"**Method:** {method_summary}",
                    "**Key Contributions:**",
                    contribution_lines,
                    "---",
                ]
            )
        )

    return "\n\n".join(blocks)


def _slim_papers_for_report(papers: list[dict]) -> list[dict]:
    """Extract only the fields needed by report/gaps/matrix agents."""
    from cached_parallel_agents import _slim_papers_for_report_impl

    return _slim_papers_for_report_impl(papers)


def run_cached_parallel_agents(
    papers: list[dict],
    domain: str,
    days: int,
) -> tuple[str, dict, str | None]:
    """Run report, gaps, and matrix agents in parallel using shared Context Cache."""
    from cached_parallel_agents import run_cached_parallel_agents_impl

    return run_cached_parallel_agents_impl(papers, domain, days)


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ğŸ“„ æŠ¥å‘Šç”Ÿæˆå™¨å¯åŠ¨")
    print("=" * 60)

    _start_time = time.time()

    # Step 1ï¼šä»æ•°æ®åº“å–è®ºæ–‡
    papers = retrieve_all_papers_from_db()

    if not papers:
        print("\nâŒ æ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œï¼špython agent.py")
    else:
        # Step 2ï¼šå¯ä¿¡åº¦è¯„åˆ† + ç½®ä¿¡åº¦åˆ¤æ–­
        print("[Report] æ­£åœ¨è¿›è¡Œå¯ä¿¡åº¦è¯„åˆ†...")
        scored_papers = [score_paper_credibility(p, DOMAIN) for p in papers]
        paper_count = len(papers)
        high_credibility_papers = [
            p for p in scored_papers if p.get("credibility_score", 0) >= CREDIBILITY_THRESHOLD
        ]
        high_credibility_count = len(high_credibility_papers)
        confidence = _confidence_level(high_credibility_count)

        print(
            f"[Report] papers: {paper_count}, high-credibility: {high_credibility_count}, "
            f"confidence: {confidence}"
        )
        print(
            f"         thresholds - trend:{MIN_PAPERS_FOR_TREND_ANALYSIS} "
            f"roadmap:{MIN_PAPERS_FOR_ROADMAP} "
            f"matrix:{MIN_PAPERS_FOR_COMPARISON}"
        )

        # Low-confidence behavior is configurable:
        # - cards_only: strict bypass with single-paper cards
        # - rich: keep full report style but skip trend/roadmap modules
        if high_credibility_count < MIN_PAPERS_FOR_TREND_ANALYSIS:
            if LOW_CONFIDENCE_REPORT_MODE == "cards_only":
                print(
                    "[Report] low-confidence hard bypass enabled: "
                    "skip trend/themes/gaps/roadmap generation."
                )
                report = render_single_paper_summary_cards(papers)
                save_report(report)
            else:
                print(
                    "[Report] low-confidence rich mode enabled: "
                    "generate report body, skip trend/roadmap modules."
                )
                _llm_fast = _make_llm(deep=False)
                _llm_deep = _make_llm(deep=True)
                print(
                    f"[Report] æ¨¡å‹é…ç½® â€” fast:{GEMINI_MODEL_FAST}  deep:{GEMINI_MODEL}(budget={THINKING_BUDGET})"
                )
                with ThreadPoolExecutor(max_workers=3) as executor:
                    fut_report = executor.submit(generate_report, papers)
                    fut_gaps = executor.submit(generate_extrapolated_gaps, papers, _llm_deep)
                    fut_matrix = None
                    if paper_count >= MIN_PAPERS_FOR_COMPARISON:
                        fut_matrix = executor.submit(render_methodology_matrix, papers, _llm_fast)
                    report = fut_report.result()
                    gaps_data = fut_gaps.result()
                    matrix_section = fut_matrix.result() if fut_matrix else None

                low_conf_notice = (
                    "## 0. Data Confidence Notice\n\n"
                    f"> Low-confidence mode: only {high_credibility_count} high-credibility paper(s), "
                    f"below required {MIN_PAPERS_FOR_TREND_ANALYSIS} for trend analysis.\n"
                    "> Trends and roadmap are skipped for reliability.\n"
                )
                report = low_conf_notice + "\n\n" + report

                if matrix_section:
                    insert_marker = "## 4."
                    if insert_marker in report:
                        report = report.replace(
                            insert_marker, matrix_section + "\n\n" + insert_marker
                        )
                    else:
                        report += "\n\n" + matrix_section

                gaps_section = render_extrapolated_gaps_markdown(gaps_data)
                report = inject_section_four(report, gaps_section)
                save_report(report, papers)

            print("\n-- Report preview (first 500 chars) --")
            print(report[:500])
            print("...\n")
            print(f"Full report saved at: {REPORT_OUTPUT_PATH}")

            _elapsed = time.time() - _start_time
            print(f"\nTotal elapsed: {_elapsed:.1f}s")

        else:
            _llm_fast = _make_llm(deep=False)
            _llm_deep = _make_llm(deep=True)
            print(
                f"[Report] æ¨¡å‹é…ç½® â€” fast:{GEMINI_MODEL_FAST}  deep:{GEMINI_MODEL}(budget={THINKING_BUDGET})"
            )

            # MEDIUM confidence: keep trend/matrix/skills (no roadmap)
            if confidence == "MEDIUM":
                print("[Report] MEDIUM confidence mode")
                with ThreadPoolExecutor(max_workers=4) as executor:
                    fut_trend = executor.submit(analyze_trends, scored_papers, _llm_deep)
                    fut_report = executor.submit(generate_report, papers)
                    fut_matrix = executor.submit(render_methodology_matrix, papers, _llm_fast)
                    fut_gaps = executor.submit(generate_extrapolated_gaps, papers, _llm_deep)
                    trend_data = fut_trend.result()
                    report = fut_report.result()
                    matrix_section = fut_matrix.result()
                    gaps_data = fut_gaps.result()

                trend_section = render_trends_markdown(trend_data)
                insert_marker = "## 1. Overview"
                if insert_marker in report:
                    report = report.replace(insert_marker, trend_section + "\n\n" + insert_marker)
                else:
                    report = trend_section + "\n\n" + report

                insert_marker_4 = "## 4."
                if insert_marker_4 in report:
                    report = report.replace(
                        insert_marker_4, matrix_section + "\n\n" + insert_marker_4
                    )
                else:
                    report += "\n\n" + matrix_section

                gaps_section = render_extrapolated_gaps_markdown(gaps_data)
                report = inject_section_four(report, gaps_section)

            # HIGH confidence: full report including roadmap
            else:
                print("[Report] HIGH confidence mode")
                with ThreadPoolExecutor(max_workers=4) as executor:
                    fut_trend = executor.submit(analyze_trends, scored_papers, _llm_deep)
                    fut_report = executor.submit(generate_report, papers)
                    fut_matrix = executor.submit(render_methodology_matrix, papers, _llm_fast)
                    fut_gaps = executor.submit(generate_extrapolated_gaps, papers, _llm_deep)
                    trend_data = fut_trend.result()
                    report = fut_report.result()
                    matrix_section = fut_matrix.result()
                    gaps_data = fut_gaps.result()

                trend_section = render_trends_markdown(trend_data)
                insert_marker = "## 1. Overview"
                if insert_marker in report:
                    report = report.replace(insert_marker, trend_section + "\n\n" + insert_marker)
                else:
                    report = trend_section + "\n\n" + report

                insert_marker_4 = "## 4."
                if insert_marker_4 in report:
                    report = report.replace(
                        insert_marker_4, matrix_section + "\n\n" + insert_marker_4
                    )
                else:
                    report += "\n\n" + matrix_section

                gaps_section = render_extrapolated_gaps_markdown(gaps_data)
                report = inject_section_four(report, gaps_section)

            # Step 3: save
            save_report(report, papers)

            # Step 4: preview
            print("\n-- Report preview (first 500 chars) --")
            print(report[:500])
            print("...\n")
            print(f"Full report saved at: {REPORT_OUTPUT_PATH}")

            _elapsed = time.time() - _start_time
            print(f"\nTotal elapsed: {_elapsed:.1f}s")
