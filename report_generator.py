# report_generator.py â€” Prompt 3ï¼šåŸºäºå‘é‡æ•°æ®åº“ç”Ÿæˆ Sparse Representation å¯¹æ¯”æŠ¥å‘Š

import json
import os
import time
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

from config import (
    GEMINI_MODEL, GEMINI_MODEL_FAST, THINKING_BUDGET,
    DOMAIN, REPORT_OUTPUT_PATH, DAYS_BACK,
    MIN_PAPERS_FOR_TREND_ANALYSIS, MIN_PAPERS_FOR_ROADMAP, MIN_PAPERS_FOR_COMPARISON,
    EMPTY_SECTION_PHRASES, CREDIBILITY_THRESHOLD, LOW_CONFIDENCE_REPORT_MODE,
)
from paper_extractor import load_db
from credibility_scorer import score_paper_credibility
from paper_extractor import load_db
from credibility_scorer import score_paper_credibility
from trend_analyzer import analyze_trends, render_trends_markdown
from skills_analyzer import (
    extract_skills_from_paper, aggregate_skills,
    generate_learning_roadmap, render_skills_markdown,
)

load_dotenv()


def _make_llm(deep: bool = False) -> ChatGoogleGenerativeAI:
    """
    LLM å·¥å‚å‡½æ•°ã€‚
    åŠ å…¥ max_retries=1 é˜²æ­¢ 429 æŠ¥é”™æ—¶æ— é™æŒ‚èµ·å¯¼è‡´ UI å¡æ­»ã€‚
    """
    if deep:
        kwargs = {
            "model": GEMINI_MODEL,
            "temperature": 1,
            "max_retries": 1,
        }
        if "thinking" in GEMINI_MODEL or "preview" in GEMINI_MODEL:
            kwargs["thinking_budget"] = THINKING_BUDGET
        return ChatGoogleGenerativeAI(**kwargs)
        
    return ChatGoogleGenerativeAI(
        model=GEMINI_MODEL_FAST,
        temperature=0.2,
        max_retries=1,
    )


# â”€â”€ Prompt 3ï¼šæŠ¥å‘Šç”Ÿæˆï¼ˆå«é˜²å¹»è§‰æŒ‡ä»¤ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

REPORT_SYSTEM_PROMPT = """You are a senior AI/signal-processing research analyst.
You will synthesize a set of recent academic paper summaries into a structured comparative report in Markdown.

ANTI-HALLUCINATION RULES (CRITICAL â€” follow these strictly):
1. Only make claims that are explicitly supported by the paper summaries provided below.
2. If you are uncertain about a fact, write "[needs verification]" instead of guessing.
3. Do not invent author names, publication dates, benchmark results, or dataset names.
4. When comparing papers, cite the paper title directly (e.g., "As shown in 'Title...', ...")
5. Do not merge findings from different papers without clearly attributing each.

Report structure (follow exactly, do not add or remove sections):

# Research Landscape: {domain} â€” Last {days} Days
*Generated: {date}*

## 1. Overview
[2-3 sentences on the overarching trend across all provided papers]

## 2. Key Themes & Breakthroughs
[Group papers by shared technical themes. For each theme:
 - Name the theme as a subheading (###)
 - Describe the core idea
 - List which papers (by title) fall under this theme
 - Highlight the advancement over prior work, based only on what the summaries say]

## 4. Identified Gaps & Open Problems
[Reserved section. Do not analyze gaps here.
 Output only: "Section generated by extrapolated gaps module."]

## 5. Recommended Reading Order
[For a researcher new to this area, suggest an order to read these papers.
 Give one sentence of justification per paper, citing its unique contribution.]

## 6. Summary Table
[End with a markdown table with columns:
 | Title (short) | Sub-domain | Method Type | Industrial Readiness | Theoretical Depth | Domain Specificity | Key Contribution |
 Fill only from the data provided.]

IMPORTANT: Do NOT generate a Section 3 â€” it will be added separately."""

REPORT_USER_TEMPLATE = """Domain: {domain}
Date range: Last {days} days

Paper Summaries (JSON):
{paper_summaries}

Write the full report now."""


GAPS_SYSTEM_PROMPT = """You are an advanced research strategist.
Given a set of recent paper summaries, infer what remains unsolved AFTER considering
the combined solutions across all papers.

Reasoning requirements (strict):
1. First synthesize the core solutions proposed across all provided papers.
2. Then perform cross-paper reasoning to find blind spots:
   edge cases, systemic issues, and interdisciplinary challenges that remain unsolved
   even if all proposed solutions work perfectly.
3. DO NOT summarize the problems the authors were trying to solve.
   You must extrapolate and deduce NEW problems or overlooked areas that emerge
   from the papers' combined methodologies.
4. STRICT RULE: If you find yourself describing a problem that one input paper
   claims to solve, discard it and find a different gap.

Output ONLY valid JSON with this exact schema:
{
  "extrapolated_gaps": [
    {
      "gap_title": "string (< 10 words)",
      "description": "string (2-3 sentences, future-looking)",
      "affected_papers": ["title1", "title2"],
      "why_unsolved": "string (1 sentence)",
      "potential_research_direction": "string (1 sentence)"
    }
  ]
}

Rules:
- Minimum 3 gaps, maximum 6 gaps.
- Each gap must be logically distinct.
- No markdown fences, no explanations outside JSON."""

GAPS_USER_TEMPLATE = """Paper Summaries (JSON):
{paper_summaries}

Generate extrapolated gaps now."""


def retrieve_all_papers_from_db() -> list[dict]:
    """ä»å‘é‡æ•°æ®åº“å–å‡ºæ‰€æœ‰å­˜å‚¨çš„è®ºæ–‡"""
    try:
        db = load_db()
        # ç”¨ä¸€ä¸ªå®½æ³›æŸ¥è¯¢å–æ‰€æœ‰è®ºæ–‡
        results = db.similarity_search("sparse representation dictionary learning", k=50)
        papers = []
        seen_ids = set()
        for doc in results:
            meta = doc.metadata
            arxiv_id = meta.get("arxiv_id", "")
            if arxiv_id in seen_ids:
                continue
            seen_ids.add(arxiv_id)
            full = json.loads(meta.get("full_json", "{}"))
            if full:
                papers.append(full)
        print(f"[Report] ä»æ•°æ®åº“å–å‡º {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    except Exception as e:
        print(f"[Report] æ•°æ®åº“è¯»å–å¤±è´¥ï¼š{e}")
        return []


def generate_report(papers: list[dict], domain: str | None = None) -> str:
    """
    è°ƒç”¨ Gemini ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šã€‚
    """
    if not papers:
        return "é”™è¯¯ï¼šæ²¡æœ‰è®ºæ–‡æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ agent.pyã€‚"

    # ç²¾ç®€ JSONï¼Œåªä¿ç•™æŠ¥å‘Šéœ€è¦çš„å­—æ®µï¼ŒèŠ‚çœ token
    slim_papers = []
    for p in papers:
        slim_papers.append({
            "title": p.get("title"),
            "published_date": p.get("published_date"),
            "sub_domain": p.get("sub_domain"),
            "problem_statement": p.get("problem_statement"),
            "method_summary": p.get("method_summary"),
            "contributions": p.get("contributions"),
            "limitations": p.get("limitations"),
            "method_keywords": p.get("method_keywords"),
            "method_type": (p.get("methodology_matrix") or {}).get("approach_type"),
            "open_source": (p.get("methodology_matrix") or {}).get("open_source"),
            "github_url": p.get("github_url"),
            "industrial_readiness_score": p.get("industrial_readiness_score"),
            "theoretical_depth": p.get("theoretical_depth"),
            "domain_specificity": p.get(
                "domain_specificity",
                p.get("relevance_to_sparse_representation"),
            ),
            "url": p.get("url"),
        })

    paper_summaries_str = json.dumps(slim_papers, ensure_ascii=False, indent=2)
    today = datetime.now().strftime("%Y-%m-%d")

    topic_domain = (domain or DOMAIN).strip() if isinstance(domain, str) else DOMAIN
    if not topic_domain:
        topic_domain = DOMAIN

    system_prompt = REPORT_SYSTEM_PROMPT.format(
        domain=topic_domain,
        days=DAYS_BACK,
        date=today,
    )
    user_prompt = REPORT_USER_TEMPLATE.format(
        domain=topic_domain,
        days=DAYS_BACK,
        paper_summaries=paper_summaries_str,
    )

    llm = _make_llm(deep=False)  # fast model â€” enough for structured report generation
    print(f"[Report] æ­£åœ¨ç”ŸæˆæŠ¥å‘Š ({GEMINI_MODEL_FAST})ï¼Œè¯·ç¨å€™...")

    response = llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt),
    ])
    return response.content


def _clean_json_response(raw: str) -> str:
    raw = raw.strip()
    if raw.startswith("```"):
        raw = raw.split("```")[1]
        if raw.startswith("json"):
            raw = raw[4:]
    return raw.strip()


def generate_extrapolated_gaps(
    papers: list[dict],
    llm: ChatGoogleGenerativeAI,
) -> dict:
    """
    Generate Section-4 gap analysis via dedicated cross-paper reasoning prompt.
    """
    slim_papers = []
    for p in papers:
        slim_papers.append({
            "title": p.get("title"),
            "sub_domain": p.get("sub_domain"),
            "problem_statement": p.get("problem_statement"),
            "method_summary": p.get("method_summary"),
            "contributions": p.get("contributions"),
            "methodology_matrix": p.get("methodology_matrix"),
        })

    user_prompt = GAPS_USER_TEMPLATE.format(
        paper_summaries=json.dumps(slim_papers, ensure_ascii=False, indent=2)
    )

    try:
        response = llm.invoke([
            SystemMessage(content=GAPS_SYSTEM_PROMPT),
            HumanMessage(content=user_prompt),
        ])
        parsed = json.loads(_clean_json_response(response.content))
        raw_gaps = parsed.get("extrapolated_gaps", [])
        if not isinstance(raw_gaps, list):
            raw_gaps = []

        normalized = []
        seen_titles = set()
        for gap in raw_gaps:
            if not isinstance(gap, dict):
                continue
            gap_title = str(gap.get("gap_title", "")).strip()
            if not gap_title:
                continue
            key = gap_title.lower()
            if key in seen_titles:
                continue
            seen_titles.add(key)

            affected = gap.get("affected_papers", [])
            if not isinstance(affected, list):
                affected = [str(affected)]
            affected = [str(t).strip() for t in affected if str(t).strip()]

            normalized.append({
                "gap_title": gap_title,
                "description": str(gap.get("description", "")).strip(),
                "affected_papers": affected,
                "why_unsolved": str(gap.get("why_unsolved", "")).strip(),
                "potential_research_direction": str(
                    gap.get("potential_research_direction", "")
                ).strip(),
            })

        return {"extrapolated_gaps": normalized[:6]}
    except Exception as e:
        print(f"[Report] extrapolated gaps generation failed: {e}")
        return {"extrapolated_gaps": []}


def render_extrapolated_gaps_markdown(gap_data: dict) -> str:
    """
    Render dedicated extrapolated gaps JSON into Section 4 markdown.
    """
    lines = [
        "## 4. Identified Gaps & Open Problems",
        "",
    ]
    gaps = gap_data.get("extrapolated_gaps", [])
    if not isinstance(gaps, list):
        gaps = []

    if not gaps:
        lines.append(
            "[needs verification] Extrapolated cross-paper gaps were not generated."
        )
        return "\n".join(lines)

    for g in gaps:
        title = g.get("gap_title") or "Untitled gap"
        desc = g.get("description") or "[needs verification]"
        affected = g.get("affected_papers") or []
        why = g.get("why_unsolved") or "[needs verification]"
        direction = g.get("potential_research_direction") or "[needs verification]"

        affected_text = ", ".join(affected) if affected else "[needs verification]"

        lines.append(f"### {title}")
        lines.append(desc)
        lines.append(f"- Affected papers: {affected_text}")
        lines.append(f"- Why unsolved: {why}")
        lines.append(f"- Potential research direction: {direction}")
        lines.append("")

    return "\n".join(lines).strip()


def inject_section_four(report_text: str, section_text: str) -> str:
    """
    Replace Section 4 with supplied markdown section.
    If missing, insert before Section 5 or append at end.
    """
    import re

    sec4_pattern = re.compile(r'## 4\..*?(?=\n## |\Z)', re.DOTALL)
    if sec4_pattern.search(report_text):
        return sec4_pattern.sub(section_text, report_text)

    sec5_pattern = re.compile(r'(?=^## 5\.)', re.MULTILINE)
    if sec5_pattern.search(report_text):
        return sec5_pattern.sub(section_text + "\n\n", report_text, count=1)

    return report_text.rstrip() + "\n\n" + section_text + "\n"


def clean_markdown_tables(text: str) -> str:
    """
    ä¿®å¤ Gemini ç”Ÿæˆçš„ Markdown è¡¨æ ¼é—®é¢˜ï¼š
    1. å°†è¿‡é•¿çš„åˆ†éš”è¡Œ | :---...--- | è§„èŒƒåŒ–ä¸º | --- |
    2. å»æ‰çº¯ç©ºç™½çš„è¶…é•¿è¡Œï¼ˆLLM æœ‰æ—¶ä¼šè¾“å‡ºæ•°åä¸‡ç©ºæ ¼ä»£æ›¿è¡¨æ ¼è¡Œï¼‰
    3. ç¡®ä¿åˆ†éš”è¡Œåˆ—æ•°ä¸ä¸Šæ–¹è¡¨å¤´è¡Œçš„åˆ—æ•°ä¸€è‡´
    """
    import re
    sep_pattern = re.compile(
        r'^(\|[ \t]*:?-{1,}:?[ \t]*)+\|?[ \t]*$'
    )
    cleaned_lines = []
    for line in text.splitlines():
        # è·³è¿‡çº¯ç©ºç™½ä¸”å¼‚å¸¸é•¿çš„è¡Œï¼ˆ> 200 å­—ç¬¦çš„çº¯ç©ºç™½ï¼‰
        if len(line) > 200 and line.strip() == '':
            continue
        if sep_pattern.match(line.strip()):
            # ä»ä¸Šä¸€è¡Œï¼ˆè¡¨å¤´è¡Œï¼‰æ¨æ–­åˆ—æ•°
            col_count = None
            if cleaned_lines:
                header = cleaned_lines[-1]
                if '|' in header:
                    cols = [c for c in header.split('|') if c.strip()]
                    col_count = len(cols)
            if col_count is None or col_count < 1:
                col_count = max(line.count('|') - 1, 1)
            cleaned_lines.append('| ' + ' | '.join(['---'] * col_count) + ' |')
        else:
            cleaned_lines.append(line)
    return '\n'.join(cleaned_lines)


def save_report(report_text: str):
    """ä¿å­˜æŠ¥å‘Šåˆ° Markdown æ–‡ä»¶"""
    report_text = clean_markdown_tables(report_text)
    with open(REPORT_OUTPUT_PATH, "w", encoding="utf-8") as f:
        f.write(report_text)
    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{REPORT_OUTPUT_PATH}")


# â”€â”€ æ™ºèƒ½ç« èŠ‚éšè— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def should_render_section(content) -> bool:
    """
    å†…å®¹é—¨æ§ï¼šè¿”å› False è¡¨ç¤ºè¯¥ç« èŠ‚åº”å®Œå…¨éšè—ã€‚
    content å¯ä»¥æ˜¯ None / ç©ºåˆ—è¡¨ / å­—ç¬¦ä¸²ã€‚
    """
    if content is None:
        return False
    if isinstance(content, list):
        return len(content) > 0
    if isinstance(content, str):
        if not content.strip():
            return False
        content_lower = content.lower()
        return not any(phrase in content_lower for phrase in EMPTY_SECTION_PHRASES)
    return True


def strip_empty_sections(report_text: str) -> str:
    """
    å¯¹ Gemini ç”Ÿæˆçš„æŠ¥å‘Šè¿›è¡Œåå¤„ç†ï¼š
    ç§»é™¤å†…å®¹ä¸­åŒ…å« EMPTY_SECTION_PHRASES çš„ Section 4ï¼ˆGapsï¼‰å—ã€‚
    å°†æ–‡æ¡£æŒ‰ ## çº§æ ‡é¢˜æ‹†åˆ†ï¼Œå¯¹ ## 4. è¿›è¡Œé—¨æ§ï¼Œå†é‡æ–°æ‹¼æ¥ã€‚
    """
    import re

    section_splitter = re.compile(r'(?=^## )', re.MULTILINE)
    chunks = section_splitter.split(report_text)

    result = []
    for chunk in chunks:
        if chunk.startswith('## 4.'):
            # æå–æ­£æ–‡ï¼ˆæ ‡é¢˜è¡Œä¹‹åï¼‰
            body_start = chunk.find('\n')
            body = chunk[body_start:] if body_start != -1 else ''
            if not should_render_section(body):
                continue  # å®Œå…¨ä¸¢å¼ƒè¿™ä¸ªå—
        result.append(chunk)

    return ''.join(result)


# â”€â”€ é˜…è¯»é¡ºåºå¢å¼ºï¼šæ³¨å…¥ arXiv é“¾æ¥ + BibTeX â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _build_bibtex(paper: dict) -> str:
    """æ ¹æ®è®ºæ–‡å…ƒæ•°æ®ç”Ÿæˆ BibTeX æ¡ç›®"""
    arxiv_id = (paper.get("arxiv_id") or "unknown").replace("/", "_")
    title = paper.get("title", "Unknown Title")
    authors = paper.get("authors") or []
    first_author = authors[0].split()[-1] if authors else "Unknown"
    year = (paper.get("published_date") or "0000")[:4]
    url = paper.get("url", "")
    return (
        f"@article{{arxiv_{arxiv_id},\n"
        f"  title={{{title}}},\n"
        f"  author={{{first_author} et al.}},\n"
        f"  year={{{year}}},\n"
        f"  url={{{url}}}\n"
        f"}}"
    )


def enhance_reading_order(report_text: str, papers: list[dict]) -> str:
    """
    åœ¨ Section 5 (Recommended Reading Order) ä¸­ï¼Œä¸ºæ¯ç¯‡è®ºæ–‡æ³¨å…¥ï¼š
    - arXiv é“¾æ¥
    - å¯æŠ˜å  BibTeX å—

    ç­–ç•¥ï¼š
    1. æå– "## 5." ç« èŠ‚å†…å®¹
    2. æŒ‰è¡Œæ‰«æï¼Œè¯†åˆ«ä»¥æ•°å­—+ç‚¹æˆ– ** å¼€å¤´çš„æ¡ç›®æ ‡é¢˜
    3. åœ¨æ¯ä¸ªæ¡ç›®æ ‡é¢˜è¡Œä¸‹æ’å…¥é“¾æ¥ + BibTeXï¼ˆé€šè¿‡æ ‡é¢˜æ¨¡ç³ŠåŒ¹é…è®ºæ–‡ï¼‰
    4. æ›¿æ¢å›åŸæ–‡æ¡£
    """
    import re
    from difflib import SequenceMatcher

    # æ„å»ºæ ‡é¢˜ â†’ è®ºæ–‡çš„æŸ¥æ‰¾ç»“æ„ï¼ˆå°å†™ï¼‰
    title_to_paper = {
        p.get("title", "").lower().strip(): p
        for p in papers if p.get("title")
    }

    def _find_paper(mention: str) -> dict | None:
        mention_lower = mention.lower().strip()
        if mention_lower in title_to_paper:
            return title_to_paper[mention_lower]
        for title, paper in title_to_paper.items():
            if mention_lower in title or title in mention_lower:
                return paper
        best, best_ratio = None, 0.0
        for title, paper in title_to_paper.items():
            ratio = SequenceMatcher(None, mention_lower, title).ratio()
            if ratio > best_ratio:
                best_ratio, best = ratio, paper
        return best if best_ratio >= 0.6 else None

    # æ‰¾åˆ° Section 5 çš„èŒƒå›´ï¼ˆç›´åˆ°ä¸‹ä¸€ä¸ª ## æ ‡é¢˜æˆ–æ–‡æ¡£ç»“å°¾ï¼‰
    sec5_match = re.search(
        r'(## 5\..*?)(?=\n## |\Z)', report_text, re.DOTALL
    )
    if not sec5_match:
        return report_text

    sec5_original = sec5_match.group(0)
    sec5_lines = sec5_original.splitlines(keepends=True)
    enhanced_lines = []

    # è¯†åˆ«æ¡ç›®è¡Œï¼šä»¥ "N. " æˆ– "**Title**" å¼€å¤´çš„è¡Œ
    entry_pattern = re.compile(r'^(\d+\.\s+|\*\*)(.+?)(\*\*)?\s*$')

    for line in sec5_lines:
        enhanced_lines.append(line)
        stripped = line.strip()
        m = entry_pattern.match(stripped)
        if m:
            candidate_title = m.group(2).strip().strip('*').strip()
            paper = _find_paper(candidate_title)
            if paper and paper.get("url"):
                url = paper["url"]
                bibtex = _build_bibtex(paper)
                enhanced_lines.append(
                    f'ğŸ”— [Read on arXiv]({url})  \n'
                    f'<details><summary>ğŸ“‹ BibTeX</summary>\n\n'
                    f'```bibtex\n{bibtex}\n```\n\n</details>\n\n'
                )

    sec5_enhanced = ''.join(enhanced_lines)
    return report_text.replace(sec5_original, sec5_enhanced)


def _sanitize_md_cell(value) -> str:
    """Normalize markdown table cell content to keep table structure valid."""
    text = str(value if value is not None else "â€”")
    text = text.replace("\n", " ").replace("\r", " ").replace("|", "\\|").strip()
    return text or "â€”"


def render_summary_table(papers: list[dict]) -> str:
    """Render Section 6 summary table from structured paper data."""
    lines = [
        "## 6. Summary Table",
        "",
        "| Title (short) | Sub-domain | Method Type | Industrial Readiness | Theoretical Depth | Domain Specificity | Key Contribution |",
        "| --- | --- | --- | --- | --- | --- | --- |",
    ]

    for p in papers:
        title = p.get("title") or "Untitled"
        title_short = title if len(title) <= 60 else f"{title[:57]}..."
        mm = p.get("methodology_matrix") or {}

        sub_domain = p.get("sub_domain") or "â€”"
        method_type = mm.get("approach_type") or "â€”"
        industrial = p.get("industrial_readiness_score")
        theory = p.get("theoretical_depth")
        specificity = p.get(
            "domain_specificity",
            p.get("relevance_to_sparse_representation"),
        )

        contributions = p.get("contributions")
        if isinstance(contributions, list) and contributions:
            key_contribution = contributions[0]
        else:
            key_contribution = "â€”"

        lines.append(
            f"| {_sanitize_md_cell(title_short)} | {_sanitize_md_cell(sub_domain)} "
            f"| {_sanitize_md_cell(method_type)} | {_sanitize_md_cell(industrial)} "
            f"| {_sanitize_md_cell(theory)} | {_sanitize_md_cell(specificity)} "
            f"| {_sanitize_md_cell(key_contribution)} |"
        )

    return "\n".join(lines)


def inject_summary_table(report_text: str, papers: list[dict]) -> str:
    """
    Replace Section 6 with a deterministic summary table built from paper data.
    If Section 6 is missing, append it at the end.
    """
    import re

    table_section = render_summary_table(papers)
    sec6_pattern = re.compile(r'## 6\. Summary Table.*?(?=\n## |\Z)', re.DOTALL)
    if sec6_pattern.search(report_text):
        return sec6_pattern.sub(table_section, report_text)
    return report_text.rstrip() + "\n\n" + table_section + "\n"


def save_report(report_text: str, papers: list[dict] | None = None):
    """ä¿å­˜æŠ¥å‘Šåˆ° Markdown æ–‡ä»¶ï¼ˆå«åå¤„ç†ï¼šè¡¨æ ¼ä¿®å¤ + ç©ºç« èŠ‚éšè— + é“¾æ¥æ³¨å…¥ï¼‰"""
    if papers:
        report_text = inject_summary_table(report_text, papers)
    report_text = clean_markdown_tables(report_text)
    report_text = strip_empty_sections(report_text)
    if papers:
        report_text = enhance_reading_order(report_text, papers)
    with open(REPORT_OUTPUT_PATH, "w", encoding="utf-8") as f:
        f.write(report_text)
    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{REPORT_OUTPUT_PATH}")



METHODOLOGY_CONTRAST_PROMPT = """You are given a methodology comparison matrix of research papers.
Write exactly 3-5 sentences that highlight the CONTRASTS between the papers.
Focus on: what makes each approach fundamentally different from the others.
Do NOT summarize individual papers. Only discuss differences.
Do NOT invent any information not present in the matrix."""


def render_methodology_matrix(papers: list[dict], llm=None) -> str:
    """
    ä»è®ºæ–‡æ•°æ®ä¸­æ„å»ºæ–¹æ³•è®ºå¯¹æ¯”çŸ©é˜µè¡¨æ ¼ + Gemini å·®å¼‚æ€§æ‘˜è¦ã€‚
    """
    lines = [
        "## 3. Methodology Comparison Matrix",
        "",
        "| Paper | Approach | Architecture | Supervision | Modality | Open Source | Theory | Key Baseline |",
        "| --- | --- | --- | --- | --- | --- | --- | --- |",
    ]

    has_missing = False
    matrix_rows_for_llm = []  # ç”¨äºç”Ÿæˆå¯¹æ¯”æ®µè½

    for p in papers:
        title = p.get("title") or "Untitled"
        title_cell = _sanitize_md_cell(title)
        mm = p.get("methodology_matrix")
        if not mm or not isinstance(mm, dict):
            lines.append(f"| {title_cell} | â€” | â€” | â€” | â€” | â€” | â€” | â€” |")
            has_missing = True
            continue

        approach = mm.get("approach_type", "â€”")
        arch = mm.get("model_architecture") or "â€”"
        supervision = mm.get("supervision_type", "â€”")
        modality = ", ".join(mm.get("data_modality", [])) or "â€”"
        oss = str(mm.get("open_source", "unknown")).lower()
        github_url = p.get("github_url") or mm.get("github_url")
        if oss == "yes" and github_url:
            open_source_display = f"âœ… [Code]({github_url})"
        elif oss == "no":
            open_source_display = "âŒ Not available"
        else:
            open_source_display = "â“ Not found"
        theory = mm.get("theoretical_guarantees", "â€”")
        baseline = mm.get("key_baseline_compared_to") or "â€”"

        lines.append(
            f"| {title_cell} | {_sanitize_md_cell(approach)} | {_sanitize_md_cell(arch)} "
            f"| {_sanitize_md_cell(supervision)} | {_sanitize_md_cell(modality)} "
            f"| {_sanitize_md_cell(open_source_display)} | {_sanitize_md_cell(theory)} "
            f"| {_sanitize_md_cell(baseline)} |"
        )
        matrix_rows_for_llm.append({
            "title": title,
            "approach": approach,
            "architecture": arch,
            "supervision": supervision,
            "modality": modality,
            "theory": theory,
        })

    lines.append("")

    if has_missing:
        lines.append(
            "> â„¹ï¸ Some papers lack methodology matrix data. "
            "Re-run `python agent.py` to extract it for all papers."
        )
        lines.append("")

    # ç”Ÿæˆå·®å¼‚æ€§æ‘˜è¦ï¼ˆ3â€“5 å¥ï¼‰
    if llm and len(matrix_rows_for_llm) >= 2:
        matrix_json = json.dumps(matrix_rows_for_llm, ensure_ascii=False, indent=2)
        try:
            print("[Report] æ­£åœ¨ç”Ÿæˆæ–¹æ³•è®ºå¯¹æ¯”æ‘˜è¦...")
            resp = llm.invoke([
                SystemMessage(content=METHODOLOGY_CONTRAST_PROMPT),
                HumanMessage(content=f"Methodology matrix:\n{matrix_json}"),
            ])
            contrast = resp.content.strip()
            lines.append(contrast)
            lines.append("")
        except Exception as e:
            print(f"[Report] å¯¹æ¯”æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{e}")

    return "\n".join(lines)


def _confidence_level(paper_count: int) -> str:
    """æ ¹æ®è®ºæ–‡æ•°é‡è¿”å›ç½®ä¿¡åº¦ç­‰çº§å­—ç¬¦ä¸²"""
    if paper_count < MIN_PAPERS_FOR_TREND_ANALYSIS:
        return "LOW"
    if paper_count < MIN_PAPERS_FOR_ROADMAP:
        return "MEDIUM"
    return "HIGH"


def _render_insufficient_data_notice(trend_data: dict) -> str:
    """æ¸²æŸ“ä½ç½®ä¿¡åº¦è­¦å‘Šæ¨ªå¹…ï¼ˆæ›¿ä»£ Section 0 è¶‹åŠ¿åˆ†æï¼‰"""
    count = trend_data.get("paper_count", 0)
    required = trend_data.get("minimum_required", MIN_PAPERS_FOR_TREND_ANALYSIS)
    available = trend_data.get("available_papers", [])

    lines = [
        "## 0. Data Confidence Notice",
        "",
        f"> âš ï¸ **Low confidence mode**: Only {count} high-credibility paper(s) found "
        f"(minimum {required} required for trend analysis).",
        ">",
        "> **What you can do:** Extend `EXTENDED_DAYS_BACK` in `config.py` to search "
        "a wider date range, or broaden your `ARXIV_QUERY`.",
        ">",
        "> **Showing:** Single-paper summaries only. "
        "Trend analysis and learning roadmap are disabled.",
        "",
    ]
    if available:
        lines.append("**Papers available in database:**")
        for t in available:
            lines.append(f"- {t}")
        lines.append("")
    return "\n".join(lines)


def render_single_paper_summary_cards(papers: list[dict]) -> str:
    """
    Low-confidence fallback: render only single-paper summary cards.
    No cross-paper sections should be included.
    """
    blocks = []
    for p in papers:
        title = p.get("title") or "Untitled"
        authors = ", ".join(p.get("authors", []) or [])
        published_date = p.get("published_date") or "unknown"
        url = p.get("url") or ""
        sub_domain = p.get("sub_domain") or "unknown"
        problem_statement = p.get("problem_statement") or "N/A"
        method_summary = p.get("method_summary") or "N/A"
        contributions = p.get("contributions") or []
        if not isinstance(contributions, list):
            contributions = [str(contributions)]
        if not contributions:
            contributions = ["N/A"]

        contribution_lines = "\n".join(
            f"- {str(item).strip()}" for item in contributions if str(item).strip()
        )
        if not contribution_lines:
            contribution_lines = "- N/A"

        blocks.append(
            "\n".join([
                "---",
                f"### {title}",
                f"**Authors:** {authors} | **Date:** {published_date} | \U0001F517 [arXiv]({url})",
                f"**Sub-domain:** {sub_domain}",
                f"**Problem:** {problem_statement}",
                f"**Method:** {method_summary}",
                "**Key Contributions:**",
                contribution_lines,
                "---",
            ])
        )

    return "\n\n".join(blocks)


if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸ“„ æŠ¥å‘Šç”Ÿæˆå™¨å¯åŠ¨")
    print("="*60)

    _start_time = time.time()

    # Step 1ï¼šä»æ•°æ®åº“å–è®ºæ–‡
    papers = retrieve_all_papers_from_db()

    if not papers:
        print("\nâŒ æ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œï¼špython agent.py")
    else:
        # Step 2ï¼šå¯ä¿¡åº¦è¯„åˆ† + ç½®ä¿¡åº¦åˆ¤æ–­
        print("[Report] æ­£åœ¨è¿›è¡Œå¯ä¿¡åº¦è¯„åˆ†...")
        scored_papers = [score_paper_credibility(p, DOMAIN) for p in papers]
        paper_count = len(papers)
        high_credibility_papers = [
            p for p in scored_papers
            if p.get("credibility_score", 0) >= CREDIBILITY_THRESHOLD
        ]
        high_credibility_count = len(high_credibility_papers)
        confidence = _confidence_level(high_credibility_count)

        print(
            f"[Report] papers: {paper_count}, high-credibility: {high_credibility_count}, "
            f"confidence: {confidence}"
        )
        print(
            f"         thresholds - trend:{MIN_PAPERS_FOR_TREND_ANALYSIS} "
            f"roadmap:{MIN_PAPERS_FOR_ROADMAP} "
            f"matrix:{MIN_PAPERS_FOR_COMPARISON}"
        )

        # Low-confidence behavior is configurable:
        # - cards_only: strict bypass with single-paper cards
        # - rich: keep full report style but skip trend/roadmap modules
        if high_credibility_count < MIN_PAPERS_FOR_TREND_ANALYSIS:
            if LOW_CONFIDENCE_REPORT_MODE == "cards_only":
                print(
                    "[Report] low-confidence hard bypass enabled: "
                    "skip trend/themes/gaps/roadmap generation."
                )
                report = render_single_paper_summary_cards(papers)
                save_report(report)
            else:
                print(
                    "[Report] low-confidence rich mode enabled: "
                    "generate report body, skip trend/roadmap modules."
                )
                _llm_fast = _make_llm(deep=False)
                _llm_deep = _make_llm(deep=True)
                print(f"[Report] æ¨¡å‹é…ç½® â€” fast:{GEMINI_MODEL_FAST}  deep:{GEMINI_MODEL}(budget={THINKING_BUDGET})")
                with ThreadPoolExecutor(max_workers=3) as executor:
                    fut_report = executor.submit(generate_report, papers)
                    fut_gaps = executor.submit(generate_extrapolated_gaps, papers, _llm_deep)
                    fut_matrix = None
                    if paper_count >= MIN_PAPERS_FOR_COMPARISON:
                        fut_matrix = executor.submit(render_methodology_matrix, papers, _llm_fast)
                    report = fut_report.result()
                    gaps_data = fut_gaps.result()
                    matrix_section = fut_matrix.result() if fut_matrix else None

                low_conf_notice = (
                    "## 0. Data Confidence Notice\n\n"
                    f"> Low-confidence mode: only {high_credibility_count} high-credibility paper(s), "
                    f"below required {MIN_PAPERS_FOR_TREND_ANALYSIS} for trend analysis.\n"
                    "> Trends and roadmap are skipped for reliability.\n"
                )
                report = low_conf_notice + "\n\n" + report

                if matrix_section:
                    insert_marker = "## 4."
                    if insert_marker in report:
                        report = report.replace(insert_marker, matrix_section + "\n\n" + insert_marker)
                    else:
                        report += "\n\n" + matrix_section

                gaps_section = render_extrapolated_gaps_markdown(gaps_data)
                report = inject_section_four(report, gaps_section)
                save_report(report, papers)

            print("\n-- Report preview (first 500 chars) --")
            print(report[:500])
            print("...\n")
            print(f"Full report saved at: {REPORT_OUTPUT_PATH}")

            _elapsed = time.time() - _start_time
            print(f"\nTotal elapsed: {_elapsed:.1f}s")

        else:
            _llm_fast = _make_llm(deep=False)
            _llm_deep = _make_llm(deep=True)
            print(f"[Report] æ¨¡å‹é…ç½® â€” fast:{GEMINI_MODEL_FAST}  deep:{GEMINI_MODEL}(budget={THINKING_BUDGET})")

            # MEDIUM confidence: keep trend/matrix/skills (no roadmap)
            if confidence == "MEDIUM":
                print("[Report] MEDIUM confidence mode")
                with ThreadPoolExecutor(max_workers=4) as executor:
                    fut_trend = executor.submit(analyze_trends, scored_papers, _llm_deep)
                    fut_report = executor.submit(generate_report, papers)
                    fut_matrix = executor.submit(render_methodology_matrix, papers, _llm_fast)
                    fut_gaps = executor.submit(generate_extrapolated_gaps, papers, _llm_deep)
                    trend_data = fut_trend.result()
                    report = fut_report.result()
                    matrix_section = fut_matrix.result()
                    gaps_data = fut_gaps.result()

                trend_section = render_trends_markdown(trend_data)
                insert_marker = "## 1. Overview"
                if insert_marker in report:
                    report = report.replace(insert_marker, trend_section + "\n\n" + insert_marker)
                else:
                    report = trend_section + "\n\n" + report

                insert_marker_4 = "## 4."
                if insert_marker_4 in report:
                    report = report.replace(insert_marker_4, matrix_section + "\n\n" + insert_marker_4)
                else:
                    report += "\n\n" + matrix_section

                gaps_section = render_extrapolated_gaps_markdown(gaps_data)
                report = inject_section_four(report, gaps_section)

                print("[Report] extracting skills in parallel...")
                with ThreadPoolExecutor(max_workers=5) as executor:
                    results = list(executor.map(
                        lambda p: extract_skills_from_paper(p, _llm_fast), papers
                    ))
                all_skills = [s for s in results if s is not None]
                if all_skills:
                    aggregated_skills = aggregate_skills(all_skills)
                    aggregated_skills["learning_roadmap"] = []
                    skills_md = render_skills_markdown(aggregated_skills)
                    unlock_note = (
                        f"\n> WARNING: **Learning roadmap locked** - requires "
                        f"{MIN_PAPERS_FOR_ROADMAP}+ papers. "
                        f"Currently {paper_count} papers analyzed. "
                        "Extend date range to unlock."
                    )
                    report += "\n\n" + skills_md + unlock_note

            # HIGH confidence: full report including roadmap
            else:
                print("[Report] HIGH confidence mode")
                with ThreadPoolExecutor(max_workers=4) as executor:
                    fut_trend = executor.submit(analyze_trends, scored_papers, _llm_deep)
                    fut_report = executor.submit(generate_report, papers)
                    fut_matrix = executor.submit(render_methodology_matrix, papers, _llm_fast)
                    fut_gaps = executor.submit(generate_extrapolated_gaps, papers, _llm_deep)
                    trend_data = fut_trend.result()
                    report = fut_report.result()
                    matrix_section = fut_matrix.result()
                    gaps_data = fut_gaps.result()

                trend_section = render_trends_markdown(trend_data)
                insert_marker = "## 1. Overview"
                if insert_marker in report:
                    report = report.replace(insert_marker, trend_section + "\n\n" + insert_marker)
                else:
                    report = trend_section + "\n\n" + report

                insert_marker_4 = "## 4."
                if insert_marker_4 in report:
                    report = report.replace(insert_marker_4, matrix_section + "\n\n" + insert_marker_4)
                else:
                    report += "\n\n" + matrix_section

                gaps_section = render_extrapolated_gaps_markdown(gaps_data)
                report = inject_section_four(report, gaps_section)

                print("[Report] extracting skills and generating roadmap...")
                with ThreadPoolExecutor(max_workers=5) as executor:
                    results = list(executor.map(
                        lambda p: extract_skills_from_paper(p, _llm_fast), papers
                    ))
                all_skills = [s for s in results if s is not None]
                if all_skills:
                    aggregated_skills = aggregate_skills(all_skills)
                    roadmap = generate_learning_roadmap(
                        aggregated_skills, _llm_fast, paper_count=paper_count
                    )
                    aggregated_skills["learning_roadmap"] = roadmap or []
                    report += "\n\n" + render_skills_markdown(aggregated_skills)
                else:
                    print("[Report] skill extraction failed, skipping skills section")

            # Step 3: save
            save_report(report, papers)

            # Step 4: preview
            print("\n-- Report preview (first 500 chars) --")
            print(report[:500])
            print("...\n")
            print(f"Full report saved at: {REPORT_OUTPUT_PATH}")

            _elapsed = time.time() - _start_time
            print(f"\nTotal elapsed: {_elapsed:.1f}s")
