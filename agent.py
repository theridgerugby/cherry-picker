# agent.py â€” Prompt 2ï¼šReAct Agentï¼Œä¸²è”æ‰€æœ‰å·¥å…·ï¼Œä¸»å…¥å£æ–‡ä»¶

import json
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.tools import tool
from langchain.agents import create_agent

from config import GEMINI_MODEL, DOMAIN, MIN_PAPERS_TO_PROCESS, DAYS_BACK
from paper_fetcher import fetch_recent_papers
from paper_extractor import extract_paper_info, store_papers_to_db, load_db
from credibility_scorer import score_paper_credibility
from trend_analyzer import analyze_trends
from skills_analyzer import extract_skills_from_paper, aggregate_skills, generate_learning_roadmap

load_dotenv()

# å…¨å±€çŠ¶æ€ï¼ˆAgent è¿è¡ŒæœŸé—´å…±äº«ï¼‰
_papers_raw = []
_papers_extracted = []
_llm = None


def get_llm():
    global _llm
    if _llm is None:
        _llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0)
    return _llm


# â”€â”€ å·¥å…·å®šä¹‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@tool
def search_arxiv(query_override: str = "") -> str:
    """
    ä» arXiv æ‹‰å–æœ€æ–°çš„ Sparse Representation ç›¸å…³è®ºæ–‡ã€‚
    è¾“å…¥ï¼šå¯é€‰çš„æŸ¥è¯¢è¯è¦†ç›–ï¼ˆç•™ç©ºåˆ™ç”¨é»˜è®¤é…ç½®ï¼‰ã€‚
    è¾“å‡ºï¼šè®ºæ–‡åˆ—è¡¨æ‘˜è¦ï¼ˆJSON å­—ç¬¦ä¸²ï¼‰ã€‚
    """
    global _papers_raw
    _papers_raw = fetch_recent_papers()
    if not _papers_raw:
        return "æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–è°ƒæ•´æœç´¢å…³é”®è¯ã€‚"
    summary = [{"index": i, "arxiv_id": p["arxiv_id"], "title": p["title"], "date": p["published_date"]}
               for i, p in enumerate(_papers_raw)]
    return json.dumps(summary, ensure_ascii=False)


@tool
def extract_and_store_paper(paper_index: str) -> str:
    """
    å¯¹æŒ‡å®šç´¢å¼•çš„è®ºæ–‡è¿›è¡Œç»“æ„åŒ–æå–ï¼Œå¹¶å­˜å…¥å‘é‡æ•°æ®åº“ã€‚
    è¾“å…¥ï¼šè®ºæ–‡åœ¨åˆ—è¡¨ä¸­çš„ indexï¼ˆæ•´æ•°ï¼‰ã€‚
    è¾“å‡ºï¼šæå–ç»“æœæ‘˜è¦ã€‚
    """
    # å…¼å®¹ Agent ä¼ å…¥ "paper_index: 0" æˆ– "0" ç­‰æ ¼å¼
    try:
        idx = int(str(paper_index).split(":")[-1].strip())
    except ValueError:
        return f"é”™è¯¯ï¼šæ— æ³•è§£æ paper_index å€¼ï¼š{paper_index}"

    global _papers_raw, _papers_extracted
    if not _papers_raw:
        return "é”™è¯¯ï¼šè¯·å…ˆè°ƒç”¨ search_arxiv è·å–è®ºæ–‡åˆ—è¡¨ã€‚"
    if idx < 0 or idx >= len(_papers_raw):
        return f"é”™è¯¯ï¼šindex {idx} è¶…å‡ºèŒƒå›´ï¼Œå…± {len(_papers_raw)} ç¯‡ã€‚"

    paper = _papers_raw[idx]
    llm = get_llm()
    result = extract_paper_info(paper, llm)

    if result is None:
        return f"æå–å¤±è´¥ï¼š{paper['title'][:60]}"

    _papers_extracted.append(result)
    return (f"æå–æˆåŠŸï¼š{result['title'][:60]}\n"
            f"  å­é¢†åŸŸï¼š{result.get('sub_domain')}\n"
            f"  å·¥ä¸šå°±ç»ªåº¦ï¼š{result.get('industrial_readiness_score')}/5\n"
            f"  ç†è®ºæ·±åº¦ï¼š{result.get('theoretical_depth')}/5\n"
            f"  é¢†åŸŸä¸“ä¸€æ€§ï¼š{result.get('domain_specificity')}/5\n"
            f"  æ ¸å¿ƒè´¡çŒ®ï¼š{result.get('contributions', [])}")


@tool
def save_all_to_database(dummy: str = "") -> str:
    """
    å°†æ‰€æœ‰å·²æå–çš„è®ºæ–‡ä¸€æ¬¡æ€§å­˜å…¥å‘é‡æ•°æ®åº“ã€‚
    åœ¨æå–å®Œæ‰€æœ‰ç›®æ ‡è®ºæ–‡åè°ƒç”¨æ­¤å·¥å…·ã€‚
    è¾“å…¥ï¼šä»»æ„å­—ç¬¦ä¸²ï¼ˆå¿½ç•¥ï¼‰ã€‚
    è¾“å‡ºï¼šå­˜å‚¨ç»“æœã€‚
    """
    global _papers_extracted
    if not _papers_extracted:
        return "æ²¡æœ‰å·²æå–çš„è®ºæ–‡ï¼Œè¯·å…ˆè°ƒç”¨ extract_and_store_paperã€‚"
    store_papers_to_db(_papers_extracted)
    return f"æˆåŠŸå°† {len(_papers_extracted)} ç¯‡è®ºæ–‡å­˜å…¥å‘é‡æ•°æ®åº“ã€‚"


@tool
def query_database(question: str) -> str:
    """
    ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ä¸é—®é¢˜æœ€ç›¸å…³çš„è®ºæ–‡ç‰‡æ®µï¼Œç”¨äºè¾…åŠ©æŠ¥å‘Šç”Ÿæˆã€‚
    è¾“å…¥ï¼šè‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œä¾‹å¦‚"å“ªäº›è®ºæ–‡ç”¨äº†å­—å…¸å­¦ä¹ æ–¹æ³•"ã€‚
    è¾“å‡ºï¼šç›¸å…³è®ºæ–‡æ‘˜è¦ï¼ˆJSONï¼‰ã€‚
    """
    try:
        db = load_db()
        results = db.similarity_search(question, k=3)
        output = []
        for doc in results:
            meta = doc.metadata
            full = json.loads(meta.get("full_json", "{}"))
            output.append({
                "title": full.get("title"),
                "sub_domain": full.get("sub_domain"),
                "contributions": full.get("contributions"),
                "method_summary": full.get("method_summary"),
            })
        return json.dumps(output, ensure_ascii=False, indent=2)
    except Exception as e:
        return f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼š{e}"


@tool
def analyze_industry_trends(dummy: str = "") -> str:
    """
    å¯¹æ•°æ®åº“ä¸­æ‰€æœ‰è®ºæ–‡è¿›è¡Œå¯ä¿¡åº¦è¯„åˆ†å¹¶ç”Ÿæˆè¡Œä¸šè¶‹åŠ¿åˆ†æã€‚
    åœ¨æå–å¹¶ä¿å­˜è®ºæ–‡åè°ƒç”¨æ­¤å·¥å…·ï¼Œè·å–å®è§‚/å¾®è§‚è¶‹åŠ¿æ´å¯Ÿã€‚
    è¾“å…¥ï¼šä»»æ„å­—ç¬¦ä¸²ï¼ˆå¿½ç•¥ï¼‰ã€‚
    è¾“å‡ºï¼šè¶‹åŠ¿åˆ†æ JSON æ‘˜è¦ã€‚
    """
    try:
        db = load_db()
        results = db.similarity_search("sparse representation", k=50)
        papers = []
        seen_ids = set()
        for doc in results:
            meta = doc.metadata
            arxiv_id = meta.get("arxiv_id", "")
            if arxiv_id in seen_ids:
                continue
            seen_ids.add(arxiv_id)
            full = json.loads(meta.get("full_json", "{}"))
            if full:
                papers.append(full)

        if not papers:
            return "é”™è¯¯ï¼šæ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆæå–è®ºæ–‡ã€‚"

        # å¯ä¿¡åº¦è¯„åˆ†
        scored_papers = [score_paper_credibility(p, DOMAIN) for p in papers]
        # è¶‹åŠ¿åˆ†æ
        llm = get_llm()
        trend_data = analyze_trends(scored_papers, llm)

        summary_lines = [f"åˆ†æäº† {len(scored_papers)} ç¯‡è®ºæ–‡"]
        macro = trend_data.get("macro_trends", [])
        if macro:
            summary_lines.append(f"å‘ç° {len(macro)} ä¸ªå®è§‚è¶‹åŠ¿ï¼š")
            for t in macro:
                summary_lines.append(f"  - {t.get('trend_name')}: {t.get('trajectory')}")
        futures = trend_data.get("future_directions", [])
        if futures:
            summary_lines.append(f"å‘ç° {len(futures)} ä¸ªæœªæ¥æ–¹å‘")

        return json.dumps(trend_data, ensure_ascii=False, indent=2)
    except Exception as e:
        return f"è¶‹åŠ¿åˆ†æå¤±è´¥ï¼š{e}"


@tool
def analyze_required_skills(dummy: str = "") -> str:
    """
    åˆ†ææ•°æ®åº“ä¸­æ‰€æœ‰è®ºæ–‡æ‰€éœ€çš„æŠ€æœ¯æŠ€èƒ½å’Œè·¨å­¦ç§‘çŸ¥è¯†ï¼Œç”Ÿæˆå­¦ä¹ è·¯çº¿å›¾ã€‚
    åœ¨ save_all_to_database ä¹‹åè°ƒç”¨æ­¤å·¥å…·ã€‚
    è¾“å…¥ï¼šä»»æ„å­—ç¬¦ä¸²ï¼ˆå¿½ç•¥ï¼‰ã€‚
    è¾“å‡ºï¼šæŠ€èƒ½åˆ†æä¸å­¦ä¹ è·¯çº¿å›¾ JSON æ‘˜è¦ã€‚
    """
    try:
        db = load_db()
        results = db.similarity_search("sparse representation", k=50)
        papers = []
        seen_ids = set()
        for doc in results:
            meta = doc.metadata
            arxiv_id = meta.get("arxiv_id", "")
            if arxiv_id in seen_ids:
                continue
            seen_ids.add(arxiv_id)
            full = json.loads(meta.get("full_json", "{}"))
            if full:
                papers.append(full)

        if not papers:
            return "é”™è¯¯ï¼šæ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆæå–è®ºæ–‡ã€‚"

        llm = get_llm()
        print(f"[Skills] æ­£åœ¨åˆ†æ {len(papers)} ç¯‡è®ºæ–‡çš„æŠ€èƒ½éœ€æ±‚...")

        # é€ç¯‡æå–æŠ€èƒ½
        all_skills = []
        for p in papers:
            skill = extract_skills_from_paper(p, llm)
            if skill:
                all_skills.append(skill)

        if not all_skills:
            return "æŠ€èƒ½æå–å¤±è´¥ï¼Œæœªèƒ½ä»ä»»ä½•è®ºæ–‡ä¸­æå–åˆ°æŠ€èƒ½ã€‚"

        # èšåˆ + è·¯çº¿å›¾
        aggregated = aggregate_skills(all_skills)
        roadmap = generate_learning_roadmap(aggregated, llm)
        aggregated["learning_roadmap"] = roadmap

        summary_lines = [f"åˆ†æäº† {len(all_skills)} ç¯‡è®ºæ–‡çš„æŠ€èƒ½éœ€æ±‚"]
        for tier in ["must_have", "important", "good_to_have"]:
            tier_data = aggregated.get(tier, {})
            total_skills = sum(len(v) for v in tier_data.values())
            summary_lines.append(f"  {tier}: {total_skills} é¡¹æŠ€èƒ½")
        summary_lines.append(f"  è·¨å­¦ç§‘é¢†åŸŸ: {len(aggregated.get('interdisciplinary_summary', []))} ä¸ª")
        summary_lines.append(f"  å­¦ä¹ è·¯çº¿å›¾: {len(roadmap)} ä¸ªé˜¶æ®µ")

        return json.dumps(aggregated, ensure_ascii=False, indent=2)
    except Exception as e:
        return f"æŠ€èƒ½åˆ†æå¤±è´¥ï¼š{e}"


AGENT_SYSTEM_PROMPT = f"""You are a research intelligence agent specialized in academic literature analysis.
Your goal is to fetch, analyze, and prepare a set of recent papers on "{DOMAIN}" for report generation.

Rules:
- After fetching papers with search_arxiv, extract AT LEAST {MIN_PAPERS_TO_PROCESS} papers using extract_and_store_paper.
- Prioritize papers with higher domain specificity to {DOMAIN} (domain_specificity 4-5).
- After extracting all target papers, call save_all_to_database.
- After saving, call analyze_industry_trends to generate trend insights.
- After trend analysis, call analyze_required_skills to generate a skills & learning roadmap.
- If a tool returns an error, try a different approach and continue.
- When all steps are complete, reply with a brief summary: how many papers found, how many processed, key themes.

Today\'s task:
Domain: {DOMAIN}
Time range: last {DAYS_BACK} days
Minimum papers to process: {MIN_PAPERS_TO_PROCESS}"""


def run_agent():
    """å¯åŠ¨ Agentï¼Œæ‰§è¡Œå®Œæ•´çš„è®ºæ–‡æ‹‰å– + æå– + å­˜å‚¨æµç¨‹"""
    llm = get_llm()
    tools = [
        search_arxiv, extract_and_store_paper, save_all_to_database,
        query_database, analyze_industry_trends, analyze_required_skills,
    ]

    agent = create_agent(
        model=llm,
        tools=tools,
        system_prompt=AGENT_SYSTEM_PROMPT,
        debug=False,
    )

    print("\n" + "="*60)
    print("ğŸš€ arXiv Agent å¯åŠ¨")
    print(f"   é¢†åŸŸï¼š{DOMAIN}")
    print(f"   ç›®æ ‡ï¼šå¤„ç†æœ€è¿‘ {DAYS_BACK} å¤©å†…è‡³å°‘ {MIN_PAPERS_TO_PROCESS} ç¯‡è®ºæ–‡")
    print("="*60 + "\n")

    result = agent.invoke({
        "messages": [{"role": "user", "content": f"Start: fetch and process papers on {DOMAIN} from the last {DAYS_BACK} days."}]
    })

    # å–æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºè¾“å‡º
    final_output = ""
    if isinstance(result, dict):
        messages = result.get("messages", [])
        if messages:
            last_msg = messages[-1]
            final_output = getattr(last_msg, "content", str(last_msg))

    print("\n" + "="*60)
    print("âœ… Agent å®Œæˆï¼")
    print(f"Final Answer: {final_output}")
    print("="*60)
    print("\nä¸‹ä¸€æ­¥ï¼šè¿è¡Œ python report_generator.py ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š")

    return _papers_extracted


if __name__ == "__main__":
    run_agent()
